'use strict';

Object.defineProperty(exports, '__esModule', { value: true });

var React = require('react');
var auth = require('aws-amplify/auth');
var react = require('@xstate/react');
var uuid = require('uuid');
var xstate = require('xstate');
var tfjsCore = require('@tensorflow/tfjs-core');
var FaceDetection$1 = require('@mediapipe/face_detection');
var tfjsConverter = require('@tensorflow/tfjs-converter');
var tfjsBackendWasm = require('@tensorflow/tfjs-backend-wasm');
require('@tensorflow/tfjs-backend-cpu');
var utils = require('@aws-amplify/core/internals/utils');
var tslib = require('tslib');
var ui = require('@aws-amplify/ui');
var clientRekognitionstreaming = require('@aws-sdk/client-rekognitionstreaming');
var utilFormatUrl = require('@aws-sdk/util-format-url');
var eventstreamSerdeBrowser = require('@smithy/eventstream-serde-browser');
var fetchHttpHandler = require('@smithy/fetch-http-handler');
var protocolHttp = require('@smithy/protocol-http');
var signatureV4 = require('@smithy/signature-v4');
var uiReact = require('@aws-amplify/ui-react');
var internal = require('@aws-amplify/ui-react/internal');

function _interopDefault (e) { return e && e.__esModule ? e : { default: e }; }

function _interopNamespace(e) {
    if (e && e.__esModule) return e;
    var n = Object.create(null);
    if (e) {
        Object.keys(e).forEach(function (k) {
            if (k !== 'default') {
                var d = Object.getOwnPropertyDescriptor(e, k);
                Object.defineProperty(n, k, d.get ? d : {
                    enumerable: true,
                    get: function () { return e[k]; }
                });
            }
        });
    }
    n["default"] = e;
    return Object.freeze(n);
}

var React__namespace = /*#__PURE__*/_interopNamespace(React);
var FaceDetection__default = /*#__PURE__*/_interopDefault(FaceDetection$1);

/**
 * The abstract class representing FaceDetection
 * to be implemented for different libraries.
 */
class FaceDetection {
    /**
     * Triggers the `loadModels` method and stores
     * the corresponding promise to be awaited later.
     */
    triggerModelLoading() {
        this.modelLoadingPromise = this.loadModels();
    }
}

/**
 * The illumination states
 */
var IlluminationState;
(function (IlluminationState) {
    IlluminationState["DARK"] = "dark";
    IlluminationState["BRIGHT"] = "bright";
    IlluminationState["NORMAL"] = "normal";
})(IlluminationState || (IlluminationState = {}));
/**
 * The detected face states with respect to the liveness oval
 */
var FaceMatchState;
(function (FaceMatchState) {
    FaceMatchState["MATCHED"] = "MATCHED";
    FaceMatchState["TOO_FAR"] = "TOO FAR";
    FaceMatchState["CANT_IDENTIFY"] = "CANNOT IDENTIFY";
    FaceMatchState["FACE_IDENTIFIED"] = "ONE FACE IDENTIFIED";
    FaceMatchState["TOO_MANY"] = "TOO MANY FACES";
    FaceMatchState["OFF_CENTER"] = "OFF CENTER";
})(FaceMatchState || (FaceMatchState = {}));

/**
 * The liveness error states
 */
const LivenessErrorState = {
    CONNECTION_TIMEOUT: 'CONNECTION_TIMEOUT',
    TIMEOUT: 'TIMEOUT',
    RUNTIME_ERROR: 'RUNTIME_ERROR',
    SERVER_ERROR: 'SERVER_ERROR',
    CAMERA_FRAMERATE_ERROR: 'CAMERA_FRAMERATE_ERROR',
    CAMERA_ACCESS_ERROR: 'CAMERA_ACCESS_ERROR',
    FACE_DISTANCE_ERROR: 'FACE_DISTANCE_ERROR',
    MOBILE_LANDSCAPE_ERROR: 'MOBILE_LANDSCAPE_ERROR',
    MULTIPLE_FACES_ERROR: 'MULTIPLE_FACES_ERROR',
    DEFAULT_CAMERA_NOT_FOUND_ERROR: 'DEFAULT_CAMERA_NOT_FOUND_ERROR',
};

// Face distance is calculated as pupilDistance / ovalWidth.
// The further away you are from the camera the distance between your pupils will decrease, thus lowering the threshold values.
// Constants from science team to determine ocular distance (space between eyes)
const PUPIL_DISTANCE_WEIGHT = 2.0;
const FACE_HEIGHT_WEIGHT = 1.8;
// Constants from science team to find face match percentage
const FACE_MATCH_RANGE_MIN = 0;
const FACE_MATCH_RANGE_MAX = 1;
const FACE_MATCH_WEIGHT_MIN = 0.25;
const FACE_MATCH_WEIGHT_MAX = 0.75;
const OVAL_HEIGHT_WIDTH_RATIO = 1.618;
const WS_CLOSURE_CODE = {
    SUCCESS_CODE: 1000,
    DEFAULT_ERROR_CODE: 4000,
    FACE_FIT_TIMEOUT: 4001,
    USER_CANCEL: 4003,
    RUNTIME_ERROR: 4005,
    USER_ERROR_DURING_CONNECTION: 4007,
};
// number in milliseconds to record into each video chunk.
// see https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder/start#timeslice
const TIME_SLICE = 1000;
// in MS, the rate at which colors are rendered/checked
const TICK_RATE = 10;
/**
 * The number of seconds before the presigned URL expires.
 * Used to override aws sdk default value of 60
 */
const REQUEST_EXPIRY = 299;
/**
 * The maximum time in milliseconds that the connection phase of a request
 * may take before the connection attempt is abandoned.
 */
const CONNECTION_TIMEOUT = 10000;
const FACE_MOVEMENT_AND_LIGHT_CHALLENGE = {
    type: 'FaceMovementAndLightChallenge',
    version: '2.0.0',
};
const FACE_MOVEMENT_CHALLENGE = {
    type: 'FaceMovementChallenge',
    version: '1.0.0',
};
const SUPPORTED_CHALLENGES = [
    FACE_MOVEMENT_AND_LIGHT_CHALLENGE,
    FACE_MOVEMENT_CHALLENGE,
];
const queryParameterString = SUPPORTED_CHALLENGES.map((challenge) => `${challenge.type}_${challenge.version}`).join(',');

/**
 * Returns the random number between min and max
 * seeded with the provided random seed.
 */
function getScaledValueFromRandomSeed(randomSeed, min, max) {
    return randomSeed * (max - min) + min;
}
/**
 * Returns the bounding box details from an oval
 */
function getOvalBoundingBox(ovalDetails) {
    const minOvalX = ovalDetails.flippedCenterX - ovalDetails.width / 2;
    const maxOvalX = ovalDetails.flippedCenterX + ovalDetails.width / 2;
    const minOvalY = ovalDetails.centerY - ovalDetails.height / 2;
    const maxOvalY = ovalDetails.centerY + ovalDetails.height / 2;
    const ovalBoundingBox = {
        left: minOvalX,
        top: minOvalY,
        right: maxOvalX,
        bottom: maxOvalY,
    };
    return { ovalBoundingBox, minOvalX, maxOvalX, minOvalY, maxOvalY };
}
/**
 * Returns the ratio of intersection and union of two bounding boxes.
 */
function getIntersectionOverUnion(box1, box2) {
    const xA = Math.max(box1.left, box2.left);
    const yA = Math.max(box1.top, box2.top);
    const xB = Math.min(box1.right, box2.right);
    const yB = Math.min(box1.bottom, box2.bottom);
    const intersectionArea = Math.abs(Math.max(0, xB - xA) * Math.max(0, yB - yA));
    if (intersectionArea === 0) {
        return 0;
    }
    const boxAArea = Math.abs((box1.right - box1.left) * (box1.bottom - box1.top));
    const boxBArea = Math.abs((box2.right - box2.left) * (box2.bottom - box2.top));
    return intersectionArea / (boxAArea + boxBArea - intersectionArea);
}
/**
 * Returns the details of a randomly generated liveness oval
 * from SDK
 */
function getOvalDetailsFromSessionInformation({ parsedSessionInformation, videoWidth, }) {
    const ovalParameters = parsedSessionInformation.Challenge.OvalParameters;
    if (!ovalParameters ||
        !ovalParameters.CenterX ||
        !ovalParameters.CenterY ||
        !ovalParameters.Width ||
        !ovalParameters.Height) {
        throw new Error('Oval parameters not returned from session information.');
    }
    // We need to include a flippedCenterX for visualizing the oval on a flipped camera view
    // The camera view we show the customer is flipped to making moving left and right more obvious
    // The video stream sent to the liveness service is not flipped
    return {
        flippedCenterX: videoWidth - ovalParameters.CenterX,
        centerX: ovalParameters.CenterX,
        centerY: ovalParameters.CenterY,
        width: ovalParameters.Width,
        height: ovalParameters.Height,
    };
}
/**
 * Returns the details of a statically generated liveness oval based on the video dimensions
 */
function getStaticLivenessOvalDetails({ width, height, widthSeed = 1.0, centerXSeed = 0.5, centerYSeed = 0.5, ratioMultiplier = 0.8, ovalHeightWidthRatio = OVAL_HEIGHT_WIDTH_RATIO, }) {
    const videoHeight = height;
    let videoWidth = width;
    const ovalRatio = widthSeed * ratioMultiplier;
    const minOvalCenterX = Math.floor((7 * width) / 16);
    const maxOvalCenterX = Math.floor((9 * width) / 16);
    const minOvalCenterY = Math.floor((7 * height) / 16);
    const maxOvalCenterY = Math.floor((9 * height) / 16);
    const centerX = getScaledValueFromRandomSeed(centerXSeed, minOvalCenterX, maxOvalCenterX);
    const centerY = getScaledValueFromRandomSeed(centerYSeed, minOvalCenterY, maxOvalCenterY);
    if (width >= height) {
        videoWidth = (3 / 4) * videoHeight;
    }
    const ovalWidth = ovalRatio * videoWidth;
    const ovalHeight = ovalHeightWidthRatio * ovalWidth;
    return {
        flippedCenterX: Math.floor(videoWidth - centerX),
        centerX: Math.floor(centerX),
        centerY: Math.floor(centerY),
        width: Math.floor(ovalWidth),
        height: Math.floor(ovalHeight),
    };
}
/**
 * Draws the provided liveness oval on the canvas.
 */
function drawLivenessOvalInCanvas({ canvas, oval, scaleFactor, videoEl, isStartScreen, }) {
    const { flippedCenterX, centerY, width, height } = oval;
    const { width: canvasWidth, height: canvasHeight } = canvas.getBoundingClientRect();
    const ctx = canvas.getContext('2d');
    if (ctx) {
        ctx.restore();
        ctx.clearRect(0, 0, canvasWidth, canvasHeight);
        // fill the canvas with a transparent rectangle
        ctx.fillStyle = isStartScreen
            ? getComputedStyle(canvas).getPropertyValue('--amplify-colors-background-primary')
            : '#fff';
        ctx.fillRect(0, 0, canvasWidth, canvasHeight);
        // On mobile our canvas is the width/height of the full screen.
        // We need to calculate horizontal and vertical translation to reposition
        // our canvas drawing so the oval is still placed relative to the dimensions
        // of the video element.
        const baseDims = { width: videoEl.videoWidth, height: videoEl.videoHeight };
        const translate = {
            x: (canvasWidth - baseDims.width * scaleFactor) / 2,
            y: (canvasHeight - baseDims.height * scaleFactor) / 2,
        };
        // Set the transform to scale
        ctx.setTransform(scaleFactor, 0, 0, scaleFactor, translate.x, translate.y);
        // draw the oval path
        ctx.beginPath();
        ctx.ellipse(flippedCenterX, centerY, width / 2, height / 2, 0, 0, 2 * Math.PI);
        // add stroke to the oval path
        ctx.strokeStyle = getComputedStyle(canvas).getPropertyValue('--amplify-colors-border-secondary');
        ctx.lineWidth = 3;
        ctx.stroke();
        ctx.save();
        ctx.clip();
        // Restore default canvas transform matrix
        ctx.setTransform(1, 0, 0, 1, 0, 0);
        // clear the oval content from the rectangle
        ctx.clearRect(0, 0, canvasWidth, canvasHeight);
    }
    else {
        throw new Error('Cannot find Canvas.');
    }
}
function drawStaticOval(canvasEl, videoEl, videoMediaStream) {
    const { width, height } = videoMediaStream.getTracks()[0].getSettings();
    // Get width/height of video element so we can compute scaleFactor
    // and set canvas width/height.
    const { width: videoScaledWidth, height: videoScaledHeight } = videoEl.getBoundingClientRect();
    canvasEl.width = Math.ceil(videoScaledWidth);
    canvasEl.height = Math.ceil(videoScaledHeight);
    const ovalDetails = getStaticLivenessOvalDetails({
        width: width,
        height: height,
        ratioMultiplier: 0.5,
    });
    ovalDetails.flippedCenterX = width - ovalDetails.centerX;
    // Compute scaleFactor which is how much our video element is scaled
    // vs the intrinsic video resolution
    const scaleFactor = videoScaledWidth / videoEl.videoWidth;
    // Draw oval in canvas using ovalDetails and scaleFactor
    drawLivenessOvalInCanvas({
        canvas: canvasEl,
        oval: ovalDetails,
        scaleFactor,
        videoEl: videoEl,
        isStartScreen: true,
    });
}
function clearOvalCanvas({ canvas, }) {
    const ctx = canvas.getContext('2d');
    if (ctx) {
        ctx.restore();
        ctx.clearRect(0, 0, Number.MAX_SAFE_INTEGER, Number.MAX_SAFE_INTEGER);
    }
    else {
        throw new Error('Cannot find Canvas.');
    }
}
function getPupilDistanceAndFaceHeight(face) {
    const { leftEye, rightEye, mouth } = face;
    const eyeCenter = [];
    eyeCenter[0] = (leftEye[0] + rightEye[0]) / 2;
    eyeCenter[1] = (leftEye[1] + rightEye[1]) / 2;
    const pupilDistance = Math.sqrt((leftEye[0] - rightEye[0]) ** 2 + (leftEye[1] - rightEye[1]) ** 2);
    const faceHeight = Math.sqrt((eyeCenter[0] - mouth[0]) ** 2 + (eyeCenter[1] - mouth[1]) ** 2);
    return { pupilDistance, faceHeight };
}
function generateBboxFromLandmarks({ ovalHeightWidthRatio = OVAL_HEIGHT_WIDTH_RATIO, face, oval, frameHeight, }) {
    const { leftEye, rightEye, nose, leftEar, rightEar } = face;
    const { height: ovalHeight, centerY } = oval;
    const ovalTop = centerY - ovalHeight / 2;
    const eyeCenter = [];
    eyeCenter[0] = (leftEye[0] + rightEye[0]) / 2;
    eyeCenter[1] = (leftEye[1] + rightEye[1]) / 2;
    const { pupilDistance: pd, faceHeight: fh } = getPupilDistanceAndFaceHeight(face);
    const ocularWidth = (PUPIL_DISTANCE_WEIGHT * pd + FACE_HEIGHT_WEIGHT * fh) / 2;
    let centerFaceX, centerFaceY;
    if (eyeCenter[1] <= (ovalTop + ovalHeight) / 2) {
        centerFaceX = (eyeCenter[0] + nose[0]) / 2;
        centerFaceY = (eyeCenter[1] + nose[1]) / 2;
    }
    else {
        // when face tilts down
        centerFaceX = eyeCenter[0];
        centerFaceY = eyeCenter[1];
    }
    const faceWidth = ocularWidth;
    const faceHeight = ovalHeightWidthRatio * faceWidth;
    const top = Math.max(centerFaceY - faceHeight / 2, 0);
    const bottom = Math.min(centerFaceY + faceHeight / 2, frameHeight);
    const left = Math.min(centerFaceX - ocularWidth / 2, rightEar[0]);
    const right = Math.max(centerFaceX + ocularWidth / 2, leftEar[0]);
    return { bottom, left, right, top };
}
/**
 * Returns the illumination state in the provided video frame.
 */
function estimateIllumination(videoEl) {
    const canvasEl = document.createElement('canvas');
    canvasEl.width = videoEl.videoWidth;
    canvasEl.height = videoEl.videoHeight;
    const ctx = canvasEl.getContext('2d');
    if (ctx) {
        ctx.drawImage(videoEl, 0, 0, canvasEl.width, canvasEl.height);
        const frame = ctx.getImageData(0, 0, canvasEl.width, canvasEl.height).data;
        // histogram
        const MAX_SCALE = 8;
        const hist = new Array(MAX_SCALE).fill(0);
        for (let i = 0; i < frame.length; i++) {
            const luma = Math.round(frame[i++] * 0.2126 + frame[i++] * 0.7152 + frame[i++] * 0.0722);
            hist[luma % 32]++;
        }
        let ind = -1, maxCount = 0;
        for (let i = 0; i < MAX_SCALE; i++) {
            if (hist[i] > maxCount) {
                maxCount = hist[i];
                ind = i;
            }
        }
        canvasEl.remove();
        if (ind === 0)
            return IlluminationState.DARK;
        if (ind === MAX_SCALE)
            return IlluminationState.BRIGHT;
        return IlluminationState.NORMAL;
    }
    else {
        throw new Error('Cannot find Video Element.');
    }
}
/**
 * Checks if the provided media device is a virtual camera.
 * @param device
 */
function isCameraDeviceVirtual(device) {
    return device.label.toLowerCase().includes('virtual');
}
const INITIAL_ALPHA = 0.9;
const SECONDARY_ALPHA = 0.75;
function fillFractionalContext({ ctx, prevColor, nextColor, fraction, }) {
    const canvasWidth = ctx.canvas.width;
    const canvasHeight = ctx.canvas.height;
    ctx.fillStyle = nextColor;
    ctx.fillRect(0, 0, canvasWidth, canvasHeight * fraction);
    if (fraction !== 1) {
        ctx.fillStyle = prevColor;
        ctx.fillRect(0, canvasHeight * fraction, canvasWidth, canvasHeight * (1 - fraction));
    }
}
function fillOverlayCanvasFractional({ overlayCanvas, prevColor, nextColor, videoEl, ovalDetails, heightFraction, scaleFactor, }) {
    const { x: videoX, y: videoY } = videoEl.getBoundingClientRect();
    const { flippedCenterX, centerY, width, height } = ovalDetails;
    const updatedCenterX = flippedCenterX * scaleFactor + videoX;
    const updatedCenterY = centerY * scaleFactor + videoY;
    const canvasWidth = overlayCanvas.width;
    const canvasHeight = overlayCanvas.height;
    const ctx = overlayCanvas.getContext('2d');
    if (ctx) {
        // Because the canvas is set to to 100% we need to manually set the height for the canvas to use pixel values
        ctx.canvas.width = window.innerWidth;
        ctx.canvas.height = window.innerHeight;
        ctx.clearRect(0, 0, canvasWidth, canvasHeight);
        // fill the complete canvas
        fillFractionalContext({
            ctx,
            prevColor,
            nextColor,
            fraction: heightFraction,
        });
        // save the current state
        ctx.save();
        // draw the rectangle path and fill it
        ctx.beginPath();
        ctx.rect(0, 0, canvasWidth, canvasHeight);
        ctx.clip();
        ctx.clearRect(0, 0, canvasWidth, canvasHeight);
        ctx.globalAlpha = INITIAL_ALPHA;
        fillFractionalContext({
            ctx,
            prevColor,
            nextColor,
            fraction: heightFraction,
        });
        // draw the oval path and fill it
        ctx.beginPath();
        ctx.ellipse(updatedCenterX, updatedCenterY, (width * scaleFactor) / 2, (height * scaleFactor) / 2, 0, 0, 2 * Math.PI);
        // add stroke to the oval path
        ctx.strokeStyle = 'white';
        ctx.lineWidth = 8;
        ctx.stroke();
        ctx.clip();
        ctx.clearRect(0, 0, canvasWidth, canvasHeight);
        ctx.globalAlpha = SECONDARY_ALPHA;
        fillFractionalContext({
            ctx,
            prevColor,
            nextColor,
            fraction: heightFraction,
        });
        // restore the state
        ctx.restore();
    }
    else {
        throw new Error('Cannot find Overlay Canvas.');
    }
}
const isColorSequence = (obj) => !!obj;
function getColorsSequencesFromSessionInformation(parsedSessionInformation) {
    const colorSequenceFromServerChallenge = parsedSessionInformation.Challenge
        .ColorSequences ?? [];
    const colorSequences = colorSequenceFromServerChallenge.map(({ FreshnessColor, DownscrollDuration: downscrollDuration, FlatDisplayDuration: flatDisplayDuration, }) => {
        const colorArray = FreshnessColor.RGB;
        const color = `rgb(${colorArray[0]},${colorArray[1]},${colorArray[2]})`;
        return typeof color !== 'undefined' &&
            typeof downscrollDuration !== 'undefined' &&
            typeof flatDisplayDuration !== 'undefined'
            ? {
                color,
                downscrollDuration,
                flatDisplayDuration,
            }
            : undefined;
    });
    return colorSequences.filter(isColorSequence);
}
async function getFaceMatchState(faceDetector, videoEl) {
    const detectedFaces = await faceDetector.detectFaces(videoEl);
    let faceMatchState;
    switch (detectedFaces.length) {
        case 0: {
            //no face detected;
            faceMatchState = FaceMatchState.CANT_IDENTIFY;
            break;
        }
        case 1: {
            //exactly one face detected, match face with oval;
            faceMatchState = FaceMatchState.FACE_IDENTIFIED;
            break;
        }
        default: {
            //more than one face detected ;
            faceMatchState = FaceMatchState.TOO_MANY;
            break;
        }
    }
    return faceMatchState;
}
async function isFaceDistanceBelowThreshold({ parsedSessionInformation, faceDetector, videoEl, ovalDetails, reduceThreshold = false, }) {
    const challengeConfig = parsedSessionInformation.Challenge.ChallengeConfig;
    const { FaceDistanceThresholdMin, FaceDistanceThreshold } = challengeConfig;
    const detectedFaces = await faceDetector.detectFaces(videoEl);
    let detectedFace;
    let isDistanceBelowThreshold = false;
    let error;
    switch (detectedFaces.length) {
        case 0: {
            //no face detected;
            error = LivenessErrorState.FACE_DISTANCE_ERROR;
            break;
        }
        case 1: {
            //exactly one face detected, match face with oval;
            detectedFace = detectedFaces[0];
            const { width } = ovalDetails;
            const { pupilDistance, faceHeight } = getPupilDistanceAndFaceHeight(detectedFace);
            const calibratedPupilDistance = (PUPIL_DISTANCE_WEIGHT * pupilDistance +
                FACE_HEIGHT_WEIGHT * faceHeight) /
                2 /
                PUPIL_DISTANCE_WEIGHT;
            if (width) {
                isDistanceBelowThreshold =
                    calibratedPupilDistance / width <
                        (!reduceThreshold
                            ? FaceDistanceThresholdMin
                            : FaceDistanceThreshold);
                if (!isDistanceBelowThreshold) {
                    error = LivenessErrorState.FACE_DISTANCE_ERROR;
                }
            }
            break;
        }
        default: {
            //more than one face detected
            error = LivenessErrorState.MULTIPLE_FACES_ERROR;
            break;
        }
    }
    return { isDistanceBelowThreshold, error };
}

/**
    * @license
    * Copyright 2024 Google LLC. All Rights Reserved.
    * Licensed under the Apache License, Version 2.0 (the "License");
    * you may not use this file except in compliance with the License.
    * You may obtain a copy of the License at
    *
    * http://www.apache.org/licenses/LICENSE-2.0
    *
    * Unless required by applicable law or agreed to in writing, software
    * distributed under the License is distributed on an "AS IS" BASIS,
    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    * See the License for the specific language governing permissions and
    * limitations under the License.
    * =============================================================================
    */
var b=function(){return b=Object.assign||function(e){for(var t,n=1,i=arguments.length;n<i;n++)for(var o in t=arguments[n])Object.prototype.hasOwnProperty.call(t,o)&&(e[o]=t[o]);return e},b.apply(this,arguments)};function T(e,t,n,i){return new(n||(n=Promise))((function(o,r){function a(e){try{h(i.next(e));}catch(e){r(e);}}function s(e){try{h(i.throw(e));}catch(e){r(e);}}function h(e){var t;e.done?o(e.value):(t=e.value,t instanceof n?t:new n((function(e){e(t);}))).then(a,s);}h((i=i.apply(e,[])).next());}))}function C(e,t){var n,i,o,r,a={label:0,sent:function(){if(1&o[0])throw o[1];return o[1]},trys:[],ops:[]};return r={next:s(0),throw:s(1),return:s(2)},"function"==typeof Symbol&&(r[Symbol.iterator]=function(){return this}),r;function s(r){return function(s){return function(r){if(n)throw new TypeError("Generator is already executing.");for(;a;)try{if(n=1,i&&(o=2&r[0]?i.return:r[0]?i.throw||((o=i.return)&&o.call(i),0):i.next)&&!(o=o.call(i,r[1])).done)return o;switch(i=0,o&&(r=[2&r[0],o.value]),r[0]){case 0:case 1:o=r;break;case 4:return a.label++,{value:r[1],done:!1};case 5:a.label++,i=r[1],r=[0];continue;case 7:r=a.ops.pop(),a.trys.pop();continue;default:if(!(o=a.trys,(o=o.length>0&&o[o.length-1])||6!==r[0]&&2!==r[0])){a=0;continue}if(3===r[0]&&(!o||r[1]>o[0]&&r[1]<o[3])){a.label=r[1];break}if(6===r[0]&&a.label<o[1]){a.label=o[1],o=r;break}if(o&&a.label<o[2]){a.label=o[2],a.ops.push(r);break}o[2]&&a.ops.pop(),a.trys.pop();continue}r=t.call(e,a);}catch(e){r=[6,e],i=0;}finally{n=o=0;}if(5&r[0])throw r[1];return {value:r[0]?r[1]:void 0,done:true}}([r,s])}}}var O=["rightEye","leftEye","noseTip","mouthCenter","rightEarTragion","leftEarTragion"];var B={modelType:"short",runtime:"mediapipe",maxFaces:1};var z=function(){function i(t){var n=this;this.width=0,this.height=0,this.selfieMode=false,this.faceDetectorSolution=new FaceDetection__default["default"]({locateFile:function(e,n){if(t.solutionPath){var i=t.solutionPath.replace(/\/+$/,"");return "".concat(i,"/").concat(e)}return "".concat(n,"/").concat(e)}}),this.faceDetectorSolution.setOptions({selfieMode:this.selfieMode,model:t.modelType}),this.faceDetectorSolution.onResults((function(e){if(n.height=e.image.height,n.width=e.image.width,n.faces=[],null!==e.detections)for(var t=0,i=e.detections;t<i.length;t++){var o=i[t];n.faces.push(n.normalizedToAbsolute(o.landmarks,(r=o.boundingBox,a=void 0,s=void 0,h=void 0,a=r.xCenter-r.width/2,s=a+r.width,h=r.yCenter-r.height/2,{xMin:a,xMax:s,yMin:h,yMax:h+r.height,width:r.width,height:r.height})));}var r,a,s,h;}));}return i.prototype.normalizedToAbsolute=function(e,t){var n=this;return {keypoints:e.map((function(e,t){return {x:e.x*n.width,y:e.y*n.height,name:O[t]}})),box:{xMin:t.xMin*this.width,yMin:t.yMin*this.height,xMax:t.xMax*this.width,yMax:t.yMax*this.height,width:t.width*this.width,height:t.height*this.height}}},i.prototype.estimateFaces=function(e,i){return T(this,void 0,void 0,(function(){var o,r;return C(this,(function(a){switch(a.label){case 0:return i&&i.flipHorizontal&&i.flipHorizontal!==this.selfieMode&&(this.selfieMode=i.flipHorizontal,this.faceDetectorSolution.setOptions({selfieMode:this.selfieMode})),e instanceof tfjsCore.Tensor?(r=ImageData.bind,[4,tfjsCore.browser.toPixels(e)]):[3,2];case 1:return o=new(r.apply(ImageData,[void 0,a.sent(),e.shape[1],e.shape[0]])),[3,3];case 2:o=e,a.label=3;case 3:return e=o,[4,this.faceDetectorSolution.send({image:e})];case 4:return a.sent(),[2,this.faces]}}))}))},i.prototype.dispose=function(){this.faceDetectorSolution.close();},i.prototype.reset=function(){this.faceDetectorSolution.reset(),this.width=0,this.height=0,this.faces=null,this.selfieMode=false;},i.prototype.initialize=function(){return this.faceDetectorSolution.initialize()},i}();function D(e){return T(this,void 0,void 0,(function(){var t,n;return C(this,(function(i){switch(i.label){case 0:return t=function(e){if(null==e)return b({},B);var t=b({},e);return t.runtime="mediapipe",null==t.modelType&&(t.modelType=B.modelType),null==t.maxFaces&&(t.maxFaces=B.maxFaces),t}(e),[4,(n=new z(t)).initialize()];case 1:return i.sent(),[2,n]}}))}))}function A(e,t,n,i){var o=e.width,r=e.height,a=1,s=Math.cos(e.rotation),h=Math.sin(e.rotation),u=e.xCenter,c=e.yCenter,l=1/t,f=1/n,d=new Array(16);return d[0]=o*s*a*l,d[1]=-r*h*l,d[2]=0,d[3]=(-0.5*o*s*a+.5*r*h+u)*l,d[4]=o*h*a*f,d[5]=r*s*f,d[6]=0,d[7]=(-0.5*r*s-.5*o*h*a+c)*f,d[8]=0,d[9]=0,d[10]=o*l,d[11]=0,d[12]=0,d[13]=0,d[14]=0,d[15]=1,function(e){if(16!==e.length)throw new Error("Array length must be 16 but got ".concat(e.length));return [[e[0],e[1],e[2],e[3]],[e[4],e[5],e[6],e[7]],[e[8],e[9],e[10],e[11]],[e[12],e[13],e[14],e[15]]]}(d)}function F(e){return e instanceof tfjsCore.Tensor?{height:e.shape[0],width:e.shape[1]}:{height:e.height,width:e.width}}function E(e){return e instanceof tfjsCore.Tensor?e:tfjsCore.browser.fromPixels(e)}function R(e,t){tfjsCore.util.assert(0!==e.width,(function(){return "".concat(t," width cannot be 0.")})),tfjsCore.util.assert(0!==e.height,(function(){return "".concat(t," height cannot be 0.")}));}function L(e,t){var n=function(e,t,n,i){var o=t-e,r=i-n;var a=r/o;return {scale:a,offset:n-e*a}}(0,255,t[0],t[1]);return tfjsCore.tidy((function(){return tfjsCore.add(tfjsCore.mul(e,n.scale),n.offset)}))}function K(e,t,n){var i=t.outputTensorSize,r=t.keepAspectRatio,a=t.borderMode,l=t.outputTensorFloatRange,f=F(e),d=function(e,t){return {xCenter:.5*e.width,yCenter:.5*e.height,width:e.width,height:e.height,rotation:0}}(f),p=function(e,t,n){if(void 0===n&&(n=false),!n)return {top:0,left:0,right:0,bottom:0};var i=t.height,o=t.width;R(t,"targetSize"),R(e,"roi");var r,a,s=i/o,h=e.height/e.width,u=0,c=0;return s>h?(r=e.width,a=e.width*s,c=(1-h/s)/2):(r=e.height/s,a=e.height,u=(1-s/h)/2),e.width=r,e.height=a,{top:c,left:u,right:u,bottom:c}}(d,i,r),m=A(d,f.width,f.height),x=tfjsCore.tidy((function(){var t=E(e),n=tfjsCore.tensor2d(function(e,t,n){return R(n,"inputResolution"),[1/n.width*e[0][0]*t.width,1/n.height*e[0][1]*t.width,e[0][3]*t.width,1/n.width*e[1][0]*t.height,1/n.height*e[1][1]*t.height,e[1][3]*t.height,0,0]}(m,f,i),[1,8]),o="zero"===a?"constant":"nearest",r=tfjsCore.image.transform(tfjsCore.expandDims(tfjsCore.cast(t,"float32")),n,"bilinear",o,0,[i.height,i.width]);return null!=l?L(r,l):r}));return {imageTensor:x,padding:p,transformationMatrix:m}}function k(e){null==e.reduceBoxesInLowestLayer&&(e.reduceBoxesInLowestLayer=false),null==e.interpolatedScaleAspectRatio&&(e.interpolatedScaleAspectRatio=1),null==e.fixedAnchorSize&&(e.fixedAnchorSize=false);for(var t=[],n=0;n<e.numLayers;){for(var i=[],o=[],r=[],a=[],s=n;s<e.strides.length&&e.strides[s]===e.strides[n];){var h=P(e.minScale,e.maxScale,s,e.strides.length);if(0===s&&e.reduceBoxesInLowestLayer)r.push(1),r.push(2),r.push(.5),a.push(.1),a.push(h),a.push(h);else {for(var u=0;u<e.aspectRatios.length;++u)r.push(e.aspectRatios[u]),a.push(h);if(e.interpolatedScaleAspectRatio>0){var c=s===e.strides.length-1?1:P(e.minScale,e.maxScale,s+1,e.strides.length);a.push(Math.sqrt(h*c)),r.push(e.interpolatedScaleAspectRatio);}}s++;}for(var l=0;l<r.length;++l){var f=Math.sqrt(r[l]);i.push(a[l]/f),o.push(a[l]*f);}var d=0,p=0;if(e.featureMapHeight.length>0)d=e.featureMapHeight[n],p=e.featureMapWidth[n];else {var m=e.strides[n];d=Math.ceil(e.inputSizeHeight/m),p=Math.ceil(e.inputSizeWidth/m);}for(var x=0;x<d;++x)for(var g=0;g<p;++g)for(var y=0;y<i.length;++y){var v={xCenter:(g+e.anchorOffsetX)/p,yCenter:(x+e.anchorOffsetY)/d,width:0,height:0};e.fixedAnchorSize?(v.width=1,v.height=1):(v.width=o[y],v.height=i[y]),t.push(v);}n=s;}return t}function P(e,t,n,i){return 1===i?.5*(e+t):e+(t-e)*n/(i-1)}function V(e,t){var n=t[0],i=t[1];return [n*e[0]+i*e[1]+e[3],n*e[4]+i*e[5]+e[7]]}function H(e){return tfjsCore.tidy((function(){var t=function(e){return tfjsCore.tidy((function(){return [tfjsCore.slice(e,[0,0,0],[1,-1,1]),tfjsCore.slice(e,[0,0,1],[1,-1,-1])]}))}(e),n=t[0],i=t[1];return {boxes:tfjsCore.squeeze(i),logits:tfjsCore.squeeze(n)}}))}function U(e,t,n,i){return T(this,void 0,void 0,(function(){var i,o,r,a,u;return C(this,(function(c){switch(c.label){case 0:return e.sort((function(e,t){return Math.max.apply(Math,t.score)-Math.max.apply(Math,e.score)})),i=tfjsCore.tensor2d(e.map((function(e){return [e.locationData.relativeBoundingBox.yMin,e.locationData.relativeBoundingBox.xMin,e.locationData.relativeBoundingBox.yMax,e.locationData.relativeBoundingBox.xMax]}))),o=tfjsCore.tensor1d(e.map((function(e){return e.score[0]}))),[4,tfjsCore.image.nonMaxSuppressionAsync(i,o,t,n)];case 1:return [4,(r=c.sent()).array()];case 2:return a=c.sent(),u=e.filter((function(e,t){return a.indexOf(t)>-1})),tfjsCore.dispose([i,o,r]),[2,u]}}))}))}function j(e,t,n){return T(this,void 0,void 0,(function(){var i,s,h,u,c;return C(this,(function(p){switch(p.label){case 0:return i=e[0],s=e[1],h=function(e,t,n){return tfjsCore.tidy((function(){var i,o,s,h;n.reverseOutputOrder?(o=tfjsCore.squeeze(tfjsCore.slice(e,[0,n.boxCoordOffset+0],[-1,1])),i=tfjsCore.squeeze(tfjsCore.slice(e,[0,n.boxCoordOffset+1],[-1,1])),h=tfjsCore.squeeze(tfjsCore.slice(e,[0,n.boxCoordOffset+2],[-1,1])),s=tfjsCore.squeeze(tfjsCore.slice(e,[0,n.boxCoordOffset+3],[-1,1]))):(i=tfjsCore.squeeze(tfjsCore.slice(e,[0,n.boxCoordOffset+0],[-1,1])),o=tfjsCore.squeeze(tfjsCore.slice(e,[0,n.boxCoordOffset+1],[-1,1])),s=tfjsCore.squeeze(tfjsCore.slice(e,[0,n.boxCoordOffset+2],[-1,1])),h=tfjsCore.squeeze(tfjsCore.slice(e,[0,n.boxCoordOffset+3],[-1,1]))),o=tfjsCore.add(tfjsCore.mul(tfjsCore.div(o,n.xScale),t.w),t.x),i=tfjsCore.add(tfjsCore.mul(tfjsCore.div(i,n.yScale),t.h),t.y),n.applyExponentialOnBoxSize?(s=tfjsCore.mul(tfjsCore.exp(tfjsCore.div(s,n.hScale)),t.h),h=tfjsCore.mul(tfjsCore.exp(tfjsCore.div(h,n.wScale)),t.w)):(s=tfjsCore.mul(tfjsCore.div(s,n.hScale),t.h),h=tfjsCore.mul(tfjsCore.div(h,n.wScale),t.h));var u=tfjsCore.sub(i,tfjsCore.div(s,2)),c=tfjsCore.sub(o,tfjsCore.div(h,2)),d=tfjsCore.add(i,tfjsCore.div(s,2)),p=tfjsCore.add(o,tfjsCore.div(h,2)),w=tfjsCore.concat([tfjsCore.reshape(u,[n.numBoxes,1]),tfjsCore.reshape(c,[n.numBoxes,1]),tfjsCore.reshape(d,[n.numBoxes,1]),tfjsCore.reshape(p,[n.numBoxes,1])],1);if(n.numKeypoints)for(var M=0;M<n.numKeypoints;++M){var S=n.keypointCoordOffset+M*n.numValuesPerKeypoint,b=void 0,T=void 0;n.reverseOutputOrder?(b=tfjsCore.squeeze(tfjsCore.slice(e,[0,S],[-1,1])),T=tfjsCore.squeeze(tfjsCore.slice(e,[0,S+1],[-1,1]))):(T=tfjsCore.squeeze(tfjsCore.slice(e,[0,S],[-1,1])),b=tfjsCore.squeeze(tfjsCore.slice(e,[0,S+1],[-1,1])));var C=tfjsCore.add(tfjsCore.mul(tfjsCore.div(b,n.xScale),t.w),t.x),O=tfjsCore.add(tfjsCore.mul(tfjsCore.div(T,n.yScale),t.h),t.y);w=tfjsCore.concat([w,tfjsCore.reshape(C,[n.numBoxes,1]),tfjsCore.reshape(O,[n.numBoxes,1])],1);}return w}))}(s,t,n),u=tfjsCore.tidy((function(){var e=i;return n.sigmoidScore?(null!=n.scoreClippingThresh&&(e=tfjsCore.clipByValue(i,-n.scoreClippingThresh,n.scoreClippingThresh)),e=tfjsCore.sigmoid(e)):e})),[4,I(h,u,n)];case 1:return c=p.sent(),tfjsCore.dispose([h,u]),[2,c]}}))}))}function I(e,t,n){return T(this,void 0,void 0,(function(){var i,o,r,a,s,h,u,c,l,f,d,p;return C(this,(function(m){switch(m.label){case 0:return i=[],[4,e.data()];case 1:return o=m.sent(),[4,t.data()];case 2:for(r=m.sent(),a=0;a<n.numBoxes;++a)if(!(null!=n.minScoreThresh&&r[a]<n.minScoreThresh||(s=a*n.numCoords,h=_(o[s+0],o[s+1],o[s+2],o[s+3],r[a],n.flipVertically,a),(u=h.locationData.relativeBoundingBox).width<0||u.height<0))){if(n.numKeypoints>0)for((c=h.locationData).relativeKeypoints=[],l=n.numKeypoints*n.numValuesPerKeypoint,f=0;f<l;f+=n.numValuesPerKeypoint)d=s+n.keypointCoordOffset+f,p={x:o[d+0],y:n.flipVertically?1-o[d+1]:o[d+1]},c.relativeKeypoints.push(p);i.push(h);}return [2,i]}}))}))}function _(e,t,n,i,o,r,a){return {score:[o],ind:a,locationData:{relativeBoundingBox:{xMin:t,yMin:r?1-n:e,xMax:i,yMax:r?1-e:n,width:i-t,height:n-e}}}}var N={reduceBoxesInLowestLayer:false,interpolatedScaleAspectRatio:1,featureMapHeight:[],featureMapWidth:[],numLayers:4,minScale:.1484375,maxScale:.75,inputSizeHeight:128,inputSizeWidth:128,anchorOffsetX:.5,anchorOffsetY:.5,strides:[8,16,16,16],aspectRatios:[1],fixedAnchorSize:true},W={reduceBoxesInLowestLayer:false,interpolatedScaleAspectRatio:0,featureMapHeight:[],featureMapWidth:[],numLayers:1,minScale:.1484375,maxScale:.75,inputSizeHeight:192,inputSizeWidth:192,anchorOffsetX:.5,anchorOffsetY:.5,strides:[4],aspectRatios:[1],fixedAnchorSize:true},X={runtime:"tfjs",modelType:"short",maxFaces:1,detectorModelUrl:"https://tfhub.dev/mediapipe/tfjs-model/face_detection/short/1"},Y={applyExponentialOnBoxSize:false,flipVertically:false,ignoreClasses:[],numClasses:1,numBoxes:896,numCoords:16,boxCoordOffset:0,keypointCoordOffset:4,numKeypoints:6,numValuesPerKeypoint:2,sigmoidScore:true,scoreClippingThresh:100,reverseOutputOrder:true,xScale:128,yScale:128,hScale:128,wScale:128,minScoreThresh:.5},q={applyExponentialOnBoxSize:false,flipVertically:false,ignoreClasses:[],numClasses:1,numBoxes:2304,numCoords:16,boxCoordOffset:0,keypointCoordOffset:4,numKeypoints:6,numValuesPerKeypoint:2,sigmoidScore:true,scoreClippingThresh:100,reverseOutputOrder:true,xScale:192,yScale:192,hScale:192,wScale:192,minScoreThresh:.6},G=.3,$={outputTensorSize:{width:128,height:128},keepAspectRatio:true,outputTensorFloatRange:[-1,1],borderMode:"zero"},J={outputTensorSize:{width:192,height:192},keepAspectRatio:true,outputTensorFloatRange:[-1,1],borderMode:"zero"};var Q,Z=function(){function e(e,t,n){this.detectorModel=t,this.maxFaces=n,"full"===e?(this.imageToTensorConfig=J,this.tensorsToDetectionConfig=q,this.anchors=k(W)):(this.imageToTensorConfig=$,this.tensorsToDetectionConfig=Y,this.anchors=k(N));var i=tfjsCore.tensor1d(this.anchors.map((function(e){return e.width}))),o=tfjsCore.tensor1d(this.anchors.map((function(e){return e.height}))),r=tfjsCore.tensor1d(this.anchors.map((function(e){return e.xCenter}))),a=tfjsCore.tensor1d(this.anchors.map((function(e){return e.yCenter})));this.anchorTensor={x:r,y:a,w:i,h:o};}return e.prototype.dispose=function(){this.detectorModel.dispose(),tfjsCore.dispose([this.anchorTensor.x,this.anchorTensor.y,this.anchorTensor.w,this.anchorTensor.h]);},e.prototype.reset=function(){},e.prototype.detectFaces=function(e,t){return void 0===t&&(t=false),T(this,void 0,void 0,(function(){var n,i,r,a,s,l,p,m,x,g,y;return C(this,(function(v){switch(v.label){case 0:return null==e?(this.reset(),[2,[]]):(n=tfjsCore.tidy((function(){var n=tfjsCore.cast(E(e),"float32");if(t){n=tfjsCore.squeeze(tfjsCore.image.flipLeftRight(tfjsCore.expandDims(n,0)),[0]);}return n})),i=K(n,this.imageToTensorConfig),r=i.imageTensor,a=i.transformationMatrix,s=this.detectorModel.execute(r,"Identity:0"),l=H(s),p=l.boxes,[4,j([m=l.logits,p],this.anchorTensor,this.tensorsToDetectionConfig)]);case 1:return 0===(x=v.sent()).length?(tfjsCore.dispose([n,r,s,m,p]),[2,x]):[4,U(x,this.maxFaces,G)];case 2:return g=v.sent(),y=function(e,t){ void 0===e&&(e=[]);var n,i=(n=t,[].concat.apply([],n));return e.forEach((function(e){var t=e.locationData;t.relativeKeypoints.forEach((function(e){var t=V(i,[e.x,e.y]),n=t[0],o=t[1];e.x=n,e.y=o;}));var n=t.relativeBoundingBox,o=Number.MAX_VALUE,r=Number.MAX_VALUE,a=Number.MIN_VALUE,s=Number.MIN_VALUE;[[n.xMin,n.yMin],[n.xMin+n.width,n.yMin],[n.xMin+n.width,n.yMin+n.height],[n.xMin,n.yMin+n.height]].forEach((function(e){var t=V(i,e),n=t[0],h=t[1];o=Math.min(o,n),a=Math.max(a,n),r=Math.min(r,h),s=Math.max(s,h);})),t.relativeBoundingBox={xMin:o,xMax:a,yMin:r,yMax:s,width:a-o,height:s-r};})),e}(g,a),tfjsCore.dispose([n,r,s,m,p]),[2,y]}}))}))},e.prototype.estimateFaces=function(e,t){return T(this,void 0,void 0,(function(){var n,i;return C(this,(function(o){return n=F(e),i=!!t&&t.flipHorizontal,[2,this.detectFaces(e,i).then((function(e){return e.map((function(e){for(var t=e.locationData.relativeKeypoints.map((function(e,t){return b(b({},e),{x:e.x*n.width,y:e.y*n.height,name:O[t]})})),i=e.locationData.relativeBoundingBox,o=0,r=["width","xMax","xMin"];o<r.length;o++){i[r[o]]*=n.width;}for(var a=0,s=["height","yMax","yMin"];a<s.length;a++){i[s[a]]*=n.height;}return {keypoints:t,box:i}}))}))]}))}))},e}();function ee(e){return T(this,void 0,void 0,(function(){var t,n,i;return C(this,(function(o){switch(o.label){case 0:return t=function(e){if(null==e)return b({},X);var t=b({},e);null==t.modelType&&(t.modelType=X.modelType),null==t.maxFaces&&(t.maxFaces=X.maxFaces),null==t.detectorModelUrl&&("full"===t.modelType?t.detectorModelUrl="https://tfhub.dev/mediapipe/tfjs-model/face_detection/full/1":t.detectorModelUrl="https://tfhub.dev/mediapipe/tfjs-model/face_detection/short/1");return t}(e),n="string"==typeof t.detectorModelUrl&&t.detectorModelUrl.indexOf("https://tfhub.dev")>-1,[4,tfjsConverter.loadGraphModel(t.detectorModelUrl,{fromTFHub:n})];case 1:return i=o.sent(),[2,new Z(t.modelType,i,t.maxFaces)]}}))}))}function te(e,t){return T(this,void 0,void 0,(function(){var n,i;return C(this,(function(o){if(e===Q.MediaPipeFaceDetector){if(i=void 0,null!=(n=t)){if("tfjs"===n.runtime)return [2,ee(n)];if("mediapipe"===n.runtime)return [2,D(n)];i=n.runtime;}throw new Error("Expect modelConfig.runtime to be either 'tfjs' "+"or 'mediapipe', but got ".concat(i))}throw new Error("".concat(e," is not a supported model name."))}))}))}!function(e){e.MediaPipeFaceDetector="MediaPipeFaceDetector";}(Q||(Q={}));

/**
 * Checks whether WebAssembly is supported in the current environment.
 */
function isWebAssemblySupported() {
    try {
        return (!!window.WebAssembly &&
            (!!window.WebAssembly.compile || !!window.WebAssembly.compileStreaming));
    }
    catch (e) {
        return false;
    }
}

const BLAZEFACE_VERSION = '1.0.2';
/**
 *   WARNING: When updating these links,
 *   also make sure to update documentation and the link in the canary/e2e test "canary/e2e/features/liveness/face-detect.feature"
 */
const DEFAULT_BLAZEFACE_URL = `https://cdn.liveness.rekognition.amazonaws.com/face-detection/tensorflow-models/blazeface/${BLAZEFACE_VERSION}/model/model.json`;
const DEFAULT_TFJS_WASM_URL = `https://cdn.liveness.rekognition.amazonaws.com/face-detection/tensorflow/tfjs-backend-wasm/${tfjsBackendWasm.version_wasm}/`;
/**
 * The BlazeFace implementation of the FaceDetection interface.
 */
class BlazeFaceFaceDetection extends FaceDetection {
    constructor(binaryPath, faceModelUrl) {
        super();
        this.faceModelUrl = faceModelUrl ?? DEFAULT_BLAZEFACE_URL;
        this.binaryPath = binaryPath ?? DEFAULT_TFJS_WASM_URL;
    }
    async loadModels() {
        if (isWebAssemblySupported()) {
            await this._loadWebAssemblyBackend();
        }
        else {
            await this._loadCPUBackend();
        }
        try {
            await tfjsCore.ready();
            this._model = await te(Q.MediaPipeFaceDetector, {
                runtime: 'tfjs',
                detectorModelUrl: this.faceModelUrl,
            });
        }
        catch (e) {
            throw new Error('There was an error loading the blazeface model. If you are using a custom blazeface model url ensure that it is a fully qualified url that returns a json file.');
        }
    }
    async detectFaces(videoEl) {
        const flipHorizontal = true;
        const predictions = await this._model.estimateFaces(videoEl, {
            flipHorizontal,
        });
        const timestampMs = Date.now();
        const faces = predictions.map((prediction) => {
            const { box, keypoints } = prediction;
            const { xMin: left, yMin: top, width, height } = box;
            const rightEye = this._getCoordinate(keypoints, 'rightEye');
            const leftEye = this._getCoordinate(keypoints, 'leftEye');
            const nose = this._getCoordinate(keypoints, 'noseTip');
            const mouth = this._getCoordinate(keypoints, 'mouthCenter');
            const rightEar = this._getCoordinate(keypoints, 'rightEarTragion');
            const leftEar = this._getCoordinate(keypoints, 'leftEarTragion');
            const probability = [90];
            return {
                top,
                left,
                width,
                height,
                timestampMs,
                probability: probability[0],
                rightEye,
                leftEye,
                mouth,
                nose,
                rightEar,
                leftEar,
            };
        });
        return faces;
    }
    _getCoordinate(keypoints, name) {
        const keypoint = keypoints.find((k) => k.name === name);
        return [keypoint.x, keypoint.y];
    }
    async _loadWebAssemblyBackend() {
        try {
            tfjsBackendWasm.setWasmPaths(this.binaryPath);
            await utils.jitteredExponentialRetry(async () => {
                const success = await tfjsCore.setBackend('wasm');
                if (!success) {
                    throw new Error(`Initialization of backend wasm failed`);
                }
            }, []);
            this.modelBackend = 'wasm';
        }
        catch (e) {
            throw new Error('There was an error loading the TFJS WASM backend. If you are using a custom WASM path ensure that it ends with "/" and that it is not the full URL as @tensorflow/tfjs-backend-wasm will append the wasm binary file name. Read more: https://github.com/tensorflow/tfjs/blob/master/tfjs-backend-wasm/src/backend_wasm.ts#L475.');
        }
    }
    async _loadCPUBackend() {
        await tfjsCore.setBackend('cpu');
        this.modelBackend = 'cpu';
    }
}

/**
 * Returns the state of the provided face with respect to the provided liveness oval.
 */
function getFaceMatchStateInLivenessOval({ face, ovalDetails, initialFaceIntersection, parsedSessionInformation, frameHeight, }) {
    let faceMatchState;
    const challengeConfig = parsedSessionInformation.Challenge.ChallengeConfig;
    if (!challengeConfig ||
        !challengeConfig.OvalIouThreshold ||
        !challengeConfig.OvalIouHeightThreshold ||
        !challengeConfig.OvalIouWidthThreshold ||
        !challengeConfig.FaceIouHeightThreshold ||
        !challengeConfig.FaceIouWidthThreshold) {
        throw new Error('Challenge information not returned from session information.');
    }
    const { OvalIouThreshold, FaceIouHeightThreshold, FaceIouWidthThreshold, OvalHeightWidthRatio, } = challengeConfig;
    const faceBoundingBox = generateBboxFromLandmarks({
        ovalHeightWidthRatio: OvalHeightWidthRatio,
        face,
        oval: ovalDetails,
        frameHeight,
    });
    const minFaceX = faceBoundingBox.left;
    const maxFaceX = faceBoundingBox.right;
    const minFaceY = faceBoundingBox.top;
    const maxFaceY = faceBoundingBox.bottom;
    const { ovalBoundingBox, minOvalX, minOvalY, maxOvalX, maxOvalY } = getOvalBoundingBox(ovalDetails);
    const intersection = getIntersectionOverUnion(faceBoundingBox, ovalBoundingBox);
    const intersectionThreshold = OvalIouThreshold;
    const faceDetectionWidthThreshold = ovalDetails.width * FaceIouWidthThreshold;
    const faceDetectionHeightThreshold = ovalDetails.height * FaceIouHeightThreshold;
    /** From Science
     * p=max(min(1,0.75∗(si−s0)/(st−s0)+0.25)),0)
     */
    const faceMatchPercentage = Math.max(Math.min(FACE_MATCH_RANGE_MAX, (FACE_MATCH_WEIGHT_MAX * (intersection - initialFaceIntersection)) /
        (intersectionThreshold - initialFaceIntersection) +
        FACE_MATCH_WEIGHT_MIN), FACE_MATCH_RANGE_MIN) * 100;
    const isFaceOutsideOvalToTheLeft = minOvalX > minFaceX && maxOvalX > maxFaceX;
    const isFaceOutsideOvalToTheRight = minFaceX > minOvalX && maxFaceX > maxOvalX;
    const isFaceMatched = intersection > intersectionThreshold;
    const isFaceMatchedClosely = minOvalY - minFaceY > faceDetectionHeightThreshold ||
        maxFaceY - maxOvalY > faceDetectionHeightThreshold ||
        (minOvalX - minFaceX > faceDetectionWidthThreshold &&
            maxFaceX - maxOvalX > faceDetectionWidthThreshold);
    if (isFaceMatched) {
        faceMatchState = FaceMatchState.MATCHED;
    }
    else if (isFaceOutsideOvalToTheLeft || isFaceOutsideOvalToTheRight) {
        faceMatchState = FaceMatchState.OFF_CENTER;
    }
    else if (isFaceMatchedClosely) {
        faceMatchState = FaceMatchState.MATCHED;
    }
    else {
        faceMatchState = FaceMatchState.TOO_FAR;
    }
    return { faceMatchState, faceMatchPercentage };
}

var _ColorSequenceDisplay_instances, _ColorSequenceDisplay_sequence, _ColorSequenceDisplay_previousSequence, _ColorSequenceDisplay_colorStage, _ColorSequenceDisplay_sequenceIndex, _ColorSequenceDisplay_colorSequences, _ColorSequenceDisplay_isFirstTick, _ColorSequenceDisplay_lastColorStageChangeTimestamp, _ColorSequenceDisplay_isFlatStage, _ColorSequenceDisplay_isScrollingStage, _ColorSequenceDisplay_startColorSequence, _ColorSequenceDisplay_handleSequenceChange;
var ColorStageType;
(function (ColorStageType) {
    ColorStageType[ColorStageType["Scrolling"] = 0] = "Scrolling";
    ColorStageType[ColorStageType["Flat"] = 1] = "Flat";
})(ColorStageType || (ColorStageType = {}));
class ColorSequenceDisplay {
    /**
     * Iterates over provided color sequences and executes sequence event callbacks
     *
     * @param {ColorSequences} colorSequences array of color sequences to iterate over
     */
    constructor(colorSequences) {
        _ColorSequenceDisplay_instances.add(this);
        /**
         * the current color sequence used for flat display and the prev color when scrolling
         */
        _ColorSequenceDisplay_sequence.set(this, void 0);
        /**
         * previous color sequence, during flat display curr === prev and during scroll it is the prev indexed color
         */
        _ColorSequenceDisplay_previousSequence.set(this, void 0);
        /**
         * current ColorStage, initialize to 'FLAT'
         */
        _ColorSequenceDisplay_colorStage.set(this, ColorStageType.Flat);
        /**
         * current color sequence index (black flat, red scrolling, etc)
         */
        _ColorSequenceDisplay_sequenceIndex.set(this, 0);
        _ColorSequenceDisplay_colorSequences.set(this, void 0);
        _ColorSequenceDisplay_isFirstTick.set(this, true);
        _ColorSequenceDisplay_lastColorStageChangeTimestamp.set(this, 0);
        tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_colorSequences, colorSequences, "f");
        tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_sequence, colorSequences[0], "f");
        tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_previousSequence, colorSequences[0], "f");
    }
    /**
     * Start sequence iteration and execute event callbacks
     *
     * @async
     * @param {StartSequencesParams} params Sequence event handlers
     * @returns {Promise<boolean>} Resolves to true when complete
     */
    async startSequences(params) {
        return new Promise((resolve) => {
            setTimeout(() => {
                tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_instances, "m", _ColorSequenceDisplay_startColorSequence).call(this, { ...params, resolve });
            }, Math.min(TICK_RATE));
        });
    }
}
_ColorSequenceDisplay_sequence = new WeakMap(), _ColorSequenceDisplay_previousSequence = new WeakMap(), _ColorSequenceDisplay_colorStage = new WeakMap(), _ColorSequenceDisplay_sequenceIndex = new WeakMap(), _ColorSequenceDisplay_colorSequences = new WeakMap(), _ColorSequenceDisplay_isFirstTick = new WeakMap(), _ColorSequenceDisplay_lastColorStageChangeTimestamp = new WeakMap(), _ColorSequenceDisplay_instances = new WeakSet(), _ColorSequenceDisplay_isFlatStage = function _ColorSequenceDisplay_isFlatStage() {
    return tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_colorStage, "f") === ColorStageType.Flat;
}, _ColorSequenceDisplay_isScrollingStage = function _ColorSequenceDisplay_isScrollingStage() {
    return tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_colorStage, "f") === ColorStageType.Scrolling;
}, _ColorSequenceDisplay_startColorSequence = function _ColorSequenceDisplay_startColorSequence({ onSequenceChange, onSequenceColorChange, onSequenceStart, onSequencesComplete, resolve, }) {
    if (ui.isFunction(onSequenceStart)) {
        onSequenceStart();
    }
    const sequenceStartTime = Date.now();
    let timeSinceLastColorStageChange = sequenceStartTime - tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_lastColorStageChangeTimestamp, "f");
    // Send a colorStart time only for the first tick of the first color
    if (tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_isFirstTick, "f")) {
        tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_lastColorStageChangeTimestamp, Date.now(), "f");
        tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_isFirstTick, false, "f");
        // initial sequence change
        if (ui.isFunction(onSequenceChange)) {
            onSequenceChange({
                prevSequenceColor: tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_previousSequence, "f").color,
                sequenceColor: tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequence, "f").color,
                sequenceIndex: tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequenceIndex, "f"),
                sequenceStartTime,
            });
        }
    }
    // Every 10 ms tick we will check if the threshold for flat or scrolling, if so we will try to go to the next stage
    if ((tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_instances, "m", _ColorSequenceDisplay_isFlatStage).call(this) &&
        timeSinceLastColorStageChange >= tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequence, "f").flatDisplayDuration) ||
        (tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_instances, "m", _ColorSequenceDisplay_isScrollingStage).call(this) &&
            timeSinceLastColorStageChange >= tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequence, "f").downscrollDuration)) {
        tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_instances, "m", _ColorSequenceDisplay_handleSequenceChange).call(this, { sequenceStartTime, onSequenceChange });
        timeSinceLastColorStageChange = 0;
    }
    const hasRemainingSequences = tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequenceIndex, "f") < tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_colorSequences, "f").length;
    // Every 10 ms tick we will update the colors displayed
    if (hasRemainingSequences) {
        const heightFraction = timeSinceLastColorStageChange /
            (tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_instances, "m", _ColorSequenceDisplay_isScrollingStage).call(this)
                ? tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequence, "f").downscrollDuration
                : tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequence, "f").flatDisplayDuration);
        if (ui.isFunction(onSequenceColorChange)) {
            onSequenceColorChange({
                sequenceColor: tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequence, "f").color,
                heightFraction,
                prevSequenceColor: tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_previousSequence, "f").color,
            });
        }
        resolve(false);
    }
    else {
        if (ui.isFunction(onSequencesComplete)) {
            onSequencesComplete();
        }
        resolve(true);
    }
}, _ColorSequenceDisplay_handleSequenceChange = function _ColorSequenceDisplay_handleSequenceChange({ sequenceStartTime, onSequenceChange, }) {
    tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_previousSequence, tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequence, "f"), "f");
    if (tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_instances, "m", _ColorSequenceDisplay_isFlatStage).call(this)) {
        tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_sequenceIndex, tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequenceIndex, "f") + 1, "f");
        tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_colorStage, ColorStageType.Scrolling, "f");
    }
    else if (tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_instances, "m", _ColorSequenceDisplay_isScrollingStage).call(this)) {
        const nextColorSequence = tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_colorSequences, "f")[tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequenceIndex, "f")];
        if (nextColorSequence.flatDisplayDuration > 0) {
            tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_colorStage, ColorStageType.Flat, "f");
        }
        else {
            tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_sequenceIndex, tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequenceIndex, "f") + 1, "f");
        }
    }
    tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_sequence, tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_colorSequences, "f")[tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequenceIndex, "f")], "f");
    tslib.__classPrivateFieldSet(this, _ColorSequenceDisplay_lastColorStageChangeTimestamp, Date.now(), "f");
    if (tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequence, "f")) {
        if (ui.isFunction(onSequenceChange)) {
            onSequenceChange({
                prevSequenceColor: tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_previousSequence, "f").color,
                sequenceColor: tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequence, "f").color,
                sequenceIndex: tslib.__classPrivateFieldGet(this, _ColorSequenceDisplay_sequenceIndex, "f"),
                sequenceStartTime: sequenceStartTime,
            });
        }
    }
};

const isFaceMovementAndLightChallenge = (value) => {
    return (value?.Challenge?.Name ===
        'FaceMovementAndLightChallenge');
};
const isFaceMovementChallenge = (value) => {
    return (value?.Challenge?.Name ===
        'FaceMovementChallenge');
};
const isFaceMovementAndLightServerChallenge = (value) => {
    return !!value
        ?.FaceMovementAndLightChallenge;
};
const isFaceMovementServerChallenge = (value) => {
    return !!value
        ?.FaceMovementChallenge;
};
const createSessionInfoFromServerSessionInformation = (serverSessionInformation) => {
    let challenge;
    if (isFaceMovementAndLightServerChallenge(serverSessionInformation.Challenge)) {
        challenge = {
            ...serverSessionInformation.Challenge.FaceMovementAndLightChallenge,
            Name: FACE_MOVEMENT_AND_LIGHT_CHALLENGE.type,
        };
    }
    else if (isFaceMovementServerChallenge(serverSessionInformation.Challenge)) {
        challenge = {
            ...serverSessionInformation.Challenge.FaceMovementChallenge,
            Name: FACE_MOVEMENT_CHALLENGE.type,
        };
    }
    else {
        throw new Error('Unsupported challenge type returned from session information.');
    }
    if (!challenge.ChallengeConfig ||
        !challenge.ChallengeConfig.FaceDistanceThreshold ||
        !challenge.ChallengeConfig.FaceDistanceThresholdMin ||
        !challenge.ChallengeConfig.OvalHeightWidthRatio) {
        throw new Error('Challenge config not returned from session information.');
    }
    return { ...serverSessionInformation, Challenge: challenge };
};

const createVideoEvent = async (result) => {
    const { data, type } = result;
    return {
        VideoChunk: new Uint8Array(
        // server expects an empty chunk on 'stopStream' event
        type === 'streamVideo' ? await data.arrayBuffer() : []),
        // @ts-expect-error for 'closeCode' event, `data` is an object which is
        // unexpected by `VideoEvent` but is expected by the streaming service
        TimestampMillis: type === 'closeCode' ? data : Date.now(),
    };
};
const getTrackDimensions = (stream) => {
    const { height: trackHeight, width: trackWidth } = stream
        .getVideoTracks()[0]
        .getSettings();
    if (ui.isUndefined(trackHeight) || ui.isUndefined(trackWidth)) {
        throw new Error(`Invalid Track Dimensions. height: ${trackHeight}, width: ${trackWidth} `);
    }
    return { trackHeight, trackWidth };
};
function getBoundingBox({ trackHeight, trackWidth, height, width, top, left, }) {
    return {
        Height: height / trackHeight,
        Width: width / trackWidth,
        Top: top / trackHeight,
        Left: left / trackWidth,
    };
}
const getFlippedInitialFaceLeft = (trackWidth, faceLeft, faceWidth) => trackWidth - faceLeft - faceWidth;
const getInitialFaceBoundingBox = (params) => {
    const { trackWidth, left, width } = params;
    return getBoundingBox({
        ...params,
        left: getFlippedInitialFaceLeft(trackWidth, left, width),
    });
};
const getTargetFaceBoundingBox = (params) => {
    const { height, width, centerX, centerY } = params;
    return getBoundingBox({
        ...params,
        top: centerY - height / 2,
        left: centerX - width / 2,
    });
};
function createClientSessionInformationEvent({ parsedSessionInformation, clientChallenge, }) {
    if (isFaceMovementChallenge(parsedSessionInformation)) {
        return {
            Challenge: {
                FaceMovementChallenge: clientChallenge,
            },
        };
    }
    if (isFaceMovementAndLightChallenge(parsedSessionInformation)) {
        return {
            Challenge: {
                FaceMovementAndLightChallenge: clientChallenge,
            },
        };
    }
    throw new Error('Unable to create ClientSessionInformationEvent');
}
function createSessionEndEvent({ parsedSessionInformation, challengeId, faceMatchAssociatedParams, ovalAssociatedParams, recordingEndedTimestamp, trackHeight, trackWidth, }) {
    const { initialFace, ovalDetails } = ovalAssociatedParams;
    const { startFace, endFace } = faceMatchAssociatedParams;
    const initialFaceBoundingBox = getInitialFaceBoundingBox({
        trackHeight,
        trackWidth,
        ...initialFace,
    });
    const targetFaceBoundingBox = getTargetFaceBoundingBox({
        trackHeight,
        trackWidth,
        ...ovalDetails,
    });
    const clientChallenge = {
        ChallengeId: challengeId,
        InitialFace: {
            InitialFaceDetectedTimestamp: initialFace.timestampMs,
            BoundingBox: initialFaceBoundingBox,
        },
        TargetFace: {
            FaceDetectedInTargetPositionStartTimestamp: startFace.timestampMs,
            FaceDetectedInTargetPositionEndTimestamp: endFace.timestampMs,
            BoundingBox: targetFaceBoundingBox,
        },
        VideoEndTimestamp: recordingEndedTimestamp,
    };
    return createClientSessionInformationEvent({
        parsedSessionInformation,
        clientChallenge,
    });
}
function createSessionStartEvent({ parsedSessionInformation, challengeId, ovalAssociatedParams, recordingStartedTimestamp, trackHeight, trackWidth, }) {
    const { initialFace } = ovalAssociatedParams;
    const initialFaceBoundingBox = getInitialFaceBoundingBox({
        trackHeight,
        trackWidth,
        ...initialFace,
    });
    const clientChallenge = {
        ChallengeId: challengeId,
        VideoStartTimestamp: recordingStartedTimestamp,
        InitialFace: {
            InitialFaceDetectedTimestamp: initialFace.timestampMs,
            BoundingBox: initialFaceBoundingBox,
        },
    };
    return createClientSessionInformationEvent({
        parsedSessionInformation,
        clientChallenge,
    });
}
/**
 * Translates provided sequence color string to an RGB array
 *
 * @param {SequenceColorValue} color
 * @returns {number[]}
 */
const colorToRgb = (color) => {
    return color
        .slice(color.indexOf('(') + 1, color.indexOf(')'))
        .split(',')
        .map((str) => parseInt(str));
};
function createColorDisplayEvent({ challengeId, sequenceStartTime, sequenceIndex, sequenceColor, prevSequenceColor, }) {
    const CurrentColor = { RGB: colorToRgb(sequenceColor) };
    const PreviousColor = {
        RGB: colorToRgb(prevSequenceColor),
    };
    return {
        Challenge: {
            FaceMovementAndLightChallenge: {
                ChallengeId: challengeId,
                ColorDisplayed: {
                    CurrentColor,
                    PreviousColor,
                    SequenceNumber: sequenceIndex,
                    CurrentColorStartTimestamp: sequenceStartTime,
                },
            },
        },
    };
}

/**
 * Creates an async generator that reads over the provided stream and yielding stream results
 *
 * @param {VideoStream} stream target video stream
 * @returns {GetRequestStream} async request stream generator
 */
function createRequestStreamGenerator(stream) {
    const reader = stream.getReader();
    return {
        getRequestStream: async function* () {
            while (true) {
                const { done, value } = await reader.read();
                if (done) {
                    return;
                }
                if (value.type === 'sessionInfo') {
                    yield { ClientSessionInformationEvent: value.data };
                }
                else {
                    // Unless value.type is closeCode we never want to send a 0 size video event as it signals end of stream
                    if (value.type === 'streamVideo' && value.data.size < 1)
                        continue;
                    yield { VideoEvent: await createVideoEvent(value) };
                }
            }
        },
    };
}

const VERSION = '3.6.0';

const BASE_USER_AGENT = `ui-react-liveness/${VERSION}`;
const getLivenessUserAgent = () => {
    return BASE_USER_AGENT;
};

/**
 * Note: This file was copied from https://github.com/aws/aws-sdk-js-v3/blob/main/packages/middleware-websocket/src/websocket-fetch-handler.ts#L176
 * Because of this the file is not fully typed at this time but we should eventually work on fully typing this file.
 */
/* eslint-disable @typescript-eslint/no-unsafe-argument */
/* eslint-disable @typescript-eslint/require-await */
/* eslint-disable @typescript-eslint/no-unsafe-call */
/* eslint-disable @typescript-eslint/no-unsafe-return */
/* eslint-disable @typescript-eslint/no-unsafe-member-access */
/* eslint-disable @typescript-eslint/no-unsafe-assignment */
const DEFAULT_WS_CONNECTION_TIMEOUT_MS = 2000;
const WEBSOCKET_CONNECTION_TIMEOUT_MESSAGE = 'Websocket connection timeout';
const isWebSocketRequest = (request) => request.protocol === 'ws:' || request.protocol === 'wss:';
const isReadableStream = (payload) => typeof ReadableStream === 'function' && payload instanceof ReadableStream;
/**
 * Transfer payload data to an AsyncIterable.
 * When the ReadableStream API is available in the runtime(e.g. browser), and
 * the request body is ReadableStream, so we need to transfer it to AsyncIterable
 * to make the stream consumable by WebSocket.
 */
const getIterator = (stream) => {
    // Noop if stream is already an async iterable
    if (stream[Symbol.asyncIterator]) {
        return stream;
    }
    if (isReadableStream(stream)) {
        // If stream is a ReadableStream, transfer the ReadableStream to async iterable.
        return eventstreamSerdeBrowser.readableStreamtoIterable(stream);
    }
    // For other types, just wrap them with an async iterable.
    return {
        [Symbol.asyncIterator]: async function* () {
            yield stream;
        },
    };
};
/**
 * Convert async iterable to a ReadableStream when ReadableStream API
 * is available(browsers). Otherwise, leave as it is(ReactNative).
 */
const toReadableStream = (asyncIterable) => typeof ReadableStream === 'function'
    ? eventstreamSerdeBrowser.iterableToReadableStream(asyncIterable)
    : asyncIterable;
/**
 * Base handler for websocket requests and HTTP request. By default, the request input and output
 * body will be in a ReadableStream, because of interface consistency among middleware.
 * If ReadableStream is not available, like in React-Native, the response body
 * will be an async iterable.
 */
class CustomWebSocketFetchHandler {
    constructor(options, httpHandler = new fetchHttpHandler.FetchHttpHandler()) {
        this.metadata = {
            handlerProtocol: 'websocket/h1.1',
        };
        this.sockets = {};
        this.utf8decoder = new TextDecoder(); // default 'utf-8' or 'utf8'
        this.httpHandler = httpHandler;
        if (typeof options === 'function') {
            this.config = {};
            this.configPromise = options().then((opts) => (this.config = opts ?? {}));
        }
        else {
            this.config = options ?? {};
            this.configPromise = Promise.resolve(this.config);
        }
    }
    /**
     * Destroys the WebSocketHandler.
     * Closes all sockets from the socket pool.
     */
    destroy() {
        for (const [key, sockets] of Object.entries(this.sockets)) {
            for (const socket of sockets) {
                socket.close(1000, `Socket closed through destroy() call`);
            }
            delete this.sockets[key];
        }
    }
    async handle(request) {
        if (!isWebSocketRequest(request)) {
            return this.httpHandler.handle(request);
        }
        const url = utilFormatUrl.formatUrl(request);
        const socket = new WebSocket(url);
        // Add socket to sockets pool
        if (!this.sockets[url]) {
            this.sockets[url] = [];
        }
        this.sockets[url].push(socket);
        socket.binaryType = 'arraybuffer';
        const { connectionTimeout = DEFAULT_WS_CONNECTION_TIMEOUT_MS } = await this.configPromise;
        await this.waitForReady(socket, connectionTimeout);
        const { body } = request;
        const bodyStream = getIterator(body);
        const asyncIterable = this.connect(socket, bodyStream);
        const outputPayload = toReadableStream(asyncIterable);
        return {
            response: new protocolHttp.HttpResponse({
                statusCode: 200,
                body: outputPayload,
            }),
        };
    }
    updateHttpClientConfig(key, value) {
        this.configPromise = this.configPromise.then((config) => {
            return {
                ...config,
                [key]: value,
            };
        });
    }
    httpHandlerConfigs() {
        return this.config ?? {};
    }
    /**
     * Removes all closing/closed sockets from the socket pool for URL.
     */
    removeNotUsableSockets(url) {
        this.sockets[url] = (this.sockets[url] ?? []).filter((socket) => !(socket.readyState === WebSocket.CLOSING ||
            socket.readyState === WebSocket.CLOSED));
    }
    waitForReady(socket, connectionTimeout) {
        return new Promise((resolve, reject) => {
            const timeout = setTimeout(() => {
                this.removeNotUsableSockets(socket.url);
                reject(new Error(WEBSOCKET_CONNECTION_TIMEOUT_MESSAGE));
            }, connectionTimeout);
            socket.onopen = () => {
                clearTimeout(timeout);
                resolve();
            };
        });
    }
    connect(socket, data) {
        // To notify output stream any error thrown after response
        // is returned while data keeps streaming.
        let streamError = undefined;
        // To notify onclose event that error has occurred.
        let socketErrorOccurred = false;
        // initialize as no-op.
        let reject = () => { };
        let resolve = () => { };
        socket.onmessage = (event) => {
            resolve({
                done: false,
                value: new Uint8Array(event.data),
            });
        };
        socket.onerror = (error) => {
            socketErrorOccurred = true;
            socket.close();
            reject(error);
        };
        socket.onclose = () => {
            this.removeNotUsableSockets(socket.url);
            if (socketErrorOccurred)
                return;
            if (streamError) {
                reject(streamError);
            }
            else {
                resolve({
                    done: true,
                    value: undefined,
                });
            }
        };
        const outputStream = {
            [Symbol.asyncIterator]: () => ({
                next: () => {
                    return new Promise((_resolve, _reject) => {
                        resolve = _resolve;
                        reject = _reject;
                    });
                },
            }),
        };
        const send = async () => {
            try {
                for await (const inputChunk of data) {
                    const decodedString = this.utf8decoder.decode(inputChunk);
                    if (decodedString.includes('closeCode')) {
                        const match = decodedString.match(/"closeCode":([0-9]*)/);
                        if (match) {
                            const closeCode = match[1];
                            socket.close(parseInt(closeCode));
                        }
                        continue;
                    }
                    socket.send(inputChunk);
                }
            }
            catch (err) {
                // We don't throw the error here because the send()'s returned
                // would already be settled by the time sending chunk throws error.
                // Instead, the notify the output stream to throw if there's
                // exceptions
                if (err instanceof Error) {
                    streamError = err;
                }
            }
            finally {
                // WS status code: https://tools.ietf.org/html/rfc6455#section-7.4
                socket.close(WS_CLOSURE_CODE.SUCCESS_CODE);
            }
        };
        send();
        return outputStream;
    }
}

const isValidCredentialsProvider = (credentialsProvider) => typeof credentialsProvider === 'function';
// the return interface of `fetchAuthSession` includes `credentials` as
// optional, but `credentials` is always returned. If `fetchAuthSession`
// is called for an unauthenticated end user, values of `accessKeyId`
// and `secretAccessKey` are `undefined`
const isCredentials = (credentials) => !!(credentials?.accessKeyId && credentials?.secretAccessKey);
/**
 * Resolves the `credentials` param to be passed to `RekognitionStreamingClient` which accepts either:
 * - a `credentials` object
 * - a `credentialsProvider` callback
 *
 * @param credentialsProvider optional `credentialsProvider` callback
 * @returns {Promise<AwsCredentials | AwsCredentialProvider>} `credentials` object or valid `credentialsProvider` callback
 */
async function resolveCredentials(credentialsProvider) {
    const hasValidCredentialsProvider = isValidCredentialsProvider(credentialsProvider);
    // provided `credentialsProvider` is not valid
    if (credentialsProvider && !hasValidCredentialsProvider) {
        throw new Error('Invalid credentialsProvider');
    }
    if (hasValidCredentialsProvider) {
        return credentialsProvider;
    }
    try {
        const result = (await auth.fetchAuthSession()).credentials;
        if (isCredentials(result)) {
            return result;
        }
        throw new Error('Missing credentials');
    }
    catch (e) {
        const { message } = e;
        throw new Error(`Invalid credentials: ${message}`);
    }
}

class Signer extends signatureV4.SignatureV4 {
    presign(request, options) {
        return super.presign(request, {
            ...options,
            expiresIn: REQUEST_EXPIRY,
            // `headers` that should not be signed. Liveness WebSocket
            // request omits `headers` except for required `host` header. Signature
            // could be a mismatch if other `headers` are signed
            unsignableHeaders: new Set(Object.keys(request.headers).filter((header) => header !== 'host')),
        });
    }
}

const DEFAULT_ATTEMPT_COUNT_TIMEOUT = 300000; // 5 minutes / 300000 ms
// Telemetry data is for internal use only and should not be depended upon or used by the customer
class TelemetryReporter {
    static getAttemptCountAndUpdateTimestamp() {
        const timeSinceLastAttempt = Date.now() - TelemetryReporter.timestamp;
        if (timeSinceLastAttempt > DEFAULT_ATTEMPT_COUNT_TIMEOUT) {
            TelemetryReporter.attemptCount = 1;
        }
        else {
            TelemetryReporter.attemptCount += 1;
        }
        TelemetryReporter.timestamp = Date.now();
        return TelemetryReporter.attemptCount;
    }
}
TelemetryReporter.attemptCount = 0;
TelemetryReporter.timestamp = Date.now();
const createTelemetryReporterMiddleware = (attemptCount, preCheckViewEnabled) => (next) => async (args) => {
    args.request.query['attempt-count'] =
        attemptCount.toString();
    args.request.query['precheck-view-enabled'] =
        preCheckViewEnabled ? '1' : '0';
    const result = await next(args);
    return result;
};

const CUSTOM_USER_AGENT = `${utils.getAmplifyUserAgent()} ${getLivenessUserAgent()}`;
async function getStreamingClient({ credentialsProvider, endpointOverride, region, systemClockOffset, }) {
    const clientconfig = {
        credentials: await resolveCredentials(credentialsProvider),
        customUserAgent: CUSTOM_USER_AGENT,
        region,
        requestHandler: new CustomWebSocketFetchHandler({
            connectionTimeout: CONNECTION_TIMEOUT,
        }),
        signerConstructor: Signer,
        systemClockOffset,
    };
    if (ui.isString(endpointOverride)) {
        clientconfig.endpointProvider = () => ({ url: new URL(endpointOverride) });
    }
    return new clientRekognitionstreaming.RekognitionStreamingClient(clientconfig);
}
const createCommandInput = ({ requestStream, sessionId, videoWidth, videoHeight, }) => ({
    ChallengeVersions: queryParameterString,
    SessionId: sessionId,
    LivenessRequestStream: requestStream,
    VideoWidth: videoWidth,
    VideoHeight: videoHeight,
});
/**
 * Initializes an instance of the Rekognition streaming client, returns `getResponseStream`
 *
 * @async
 * @param clientConfig configuration fpr the client
 * @returns {Promise<{ getResponseStream: GetReponseStream }>}
 */
async function createStreamingClient(clientConfig) {
    const client = await getStreamingClient(clientConfig);
    client.middlewareStack.add(createTelemetryReporterMiddleware(clientConfig.attemptCount, clientConfig.preCheckViewEnabled), {
        step: 'build',
        name: 'telemetryMiddleware',
        tags: ['liveness', 'amplify-ui'],
    });
    return {
        async getResponseStream(input) {
            const command = new clientRekognitionstreaming.StartFaceLivenessSessionCommand(createCommandInput(input));
            const { LivenessResponseStream } = await client.send(command);
            return LivenessResponseStream;
        },
    };
}

var _StreamRecorder_instances, _StreamRecorder_chunks, _StreamRecorder_recorder, _StreamRecorder_initialRecorder, _StreamRecorder_recordingStarted, _StreamRecorder_firstChunkTimestamp, _StreamRecorder_recorderEndTimestamp, _StreamRecorder_recorderStartTimestamp, _StreamRecorder_recordingStartTimestamp, _StreamRecorder_recorderStopped, _StreamRecorder_videoStream, _StreamRecorder_eventListeners, _StreamRecorder_clearRecordedChunks, _StreamRecorder_createReadableStream, _StreamRecorder_attachHandlers, _StreamRecorder_setupCallbacks, _StreamRecorder_cleanUpEventListeners;
class StreamRecorder {
    constructor(stream) {
        _StreamRecorder_instances.add(this);
        _StreamRecorder_chunks.set(this, void 0);
        _StreamRecorder_recorder.set(this, void 0);
        _StreamRecorder_initialRecorder.set(this, void 0);
        _StreamRecorder_recordingStarted.set(this, false);
        _StreamRecorder_firstChunkTimestamp.set(this, void 0);
        _StreamRecorder_recorderEndTimestamp.set(this, void 0);
        _StreamRecorder_recorderStartTimestamp.set(this, void 0);
        _StreamRecorder_recordingStartTimestamp.set(this, void 0);
        _StreamRecorder_recorderStopped.set(this, void 0);
        _StreamRecorder_videoStream.set(this, void 0);
        _StreamRecorder_eventListeners.set(this, void 0);
        if (typeof MediaRecorder === 'undefined') {
            throw Error('MediaRecorder is not supported by this browser');
        }
        tslib.__classPrivateFieldSet(this, _StreamRecorder_chunks, [], "f");
        tslib.__classPrivateFieldSet(this, _StreamRecorder_recorder, new MediaRecorder(stream, { bitsPerSecond: 1000000 }), "f");
        tslib.__classPrivateFieldSet(this, _StreamRecorder_initialRecorder, tslib.__classPrivateFieldGet(this, _StreamRecorder_recorder, "f"), "f");
        tslib.__classPrivateFieldSet(this, _StreamRecorder_videoStream, tslib.__classPrivateFieldGet(this, _StreamRecorder_instances, "m", _StreamRecorder_createReadableStream).call(this), "f");
        tslib.__classPrivateFieldSet(this, _StreamRecorder_eventListeners, {}, "f");
    }
    getVideoStream() {
        return tslib.__classPrivateFieldGet(this, _StreamRecorder_videoStream, "f");
    }
    setNewVideoStream(stream) {
        tslib.__classPrivateFieldGet(this, _StreamRecorder_instances, "m", _StreamRecorder_cleanUpEventListeners).call(this);
        tslib.__classPrivateFieldSet(this, _StreamRecorder_recorder, new MediaRecorder(stream, { bitsPerSecond: 1000000 }), "f");
        tslib.__classPrivateFieldGet(this, _StreamRecorder_instances, "m", _StreamRecorder_attachHandlers).call(this, tslib.__classPrivateFieldGet(this, _StreamRecorder_recorder, "f"));
    }
    dispatchStreamEvent(event) {
        const { type } = event;
        const data = type === 'streamStop' ? undefined : event.data;
        tslib.__classPrivateFieldGet(this, _StreamRecorder_recorder, "f").dispatchEvent(new MessageEvent(type, { data }));
    }
    getRecordingStartTimestamp() {
        if (ui.isUndefined(tslib.__classPrivateFieldGet(this, _StreamRecorder_recorderStartTimestamp, "f")) ||
            ui.isUndefined(tslib.__classPrivateFieldGet(this, _StreamRecorder_recordingStartTimestamp, "f"))) {
            throw new Error('Recording has not started');
        }
        /**
         * This calculation is provided by Science team after doing analysis
         * of unreliable .onstart() (this.#recorderStartTimestamp) timestamp that is
         * returned from mediaRecorder.
         */
        return Math.round(0.73 * (tslib.__classPrivateFieldGet(this, _StreamRecorder_recorderStartTimestamp, "f") - tslib.__classPrivateFieldGet(this, _StreamRecorder_recordingStartTimestamp, "f")) +
            tslib.__classPrivateFieldGet(this, _StreamRecorder_recordingStartTimestamp, "f"));
    }
    getRecordingEndedTimestamp() {
        if (ui.isUndefined(tslib.__classPrivateFieldGet(this, _StreamRecorder_recorderEndTimestamp, "f"))) {
            throw new Error('Recording has not ended');
        }
        return tslib.__classPrivateFieldGet(this, _StreamRecorder_recorderEndTimestamp, "f");
    }
    startRecording() {
        tslib.__classPrivateFieldGet(this, _StreamRecorder_instances, "m", _StreamRecorder_clearRecordedChunks).call(this);
        tslib.__classPrivateFieldSet(this, _StreamRecorder_recordingStartTimestamp, Date.now(), "f");
        tslib.__classPrivateFieldGet(this, _StreamRecorder_recorder, "f").start(TIME_SLICE);
    }
    isRecording() {
        return tslib.__classPrivateFieldGet(this, _StreamRecorder_recorder, "f").state === 'recording';
    }
    getChunksLength() {
        return tslib.__classPrivateFieldGet(this, _StreamRecorder_chunks, "f").length;
    }
    hasRecordingStarted() {
        return tslib.__classPrivateFieldGet(this, _StreamRecorder_recordingStarted, "f") && tslib.__classPrivateFieldGet(this, _StreamRecorder_firstChunkTimestamp, "f") !== undefined;
    }
    async stopRecording() {
        if (this.isRecording()) {
            tslib.__classPrivateFieldGet(this, _StreamRecorder_recorder, "f").stop();
        }
        return tslib.__classPrivateFieldGet(this, _StreamRecorder_recorderStopped, "f");
    }
}
_StreamRecorder_chunks = new WeakMap(), _StreamRecorder_recorder = new WeakMap(), _StreamRecorder_initialRecorder = new WeakMap(), _StreamRecorder_recordingStarted = new WeakMap(), _StreamRecorder_firstChunkTimestamp = new WeakMap(), _StreamRecorder_recorderEndTimestamp = new WeakMap(), _StreamRecorder_recorderStartTimestamp = new WeakMap(), _StreamRecorder_recordingStartTimestamp = new WeakMap(), _StreamRecorder_recorderStopped = new WeakMap(), _StreamRecorder_videoStream = new WeakMap(), _StreamRecorder_eventListeners = new WeakMap(), _StreamRecorder_instances = new WeakSet(), _StreamRecorder_clearRecordedChunks = function _StreamRecorder_clearRecordedChunks() {
    tslib.__classPrivateFieldSet(this, _StreamRecorder_chunks, [], "f");
}, _StreamRecorder_createReadableStream = function _StreamRecorder_createReadableStream() {
    return new ReadableStream({
        start: (controller) => {
            tslib.__classPrivateFieldGet(this, _StreamRecorder_instances, "m", _StreamRecorder_attachHandlers).call(this, tslib.__classPrivateFieldGet(this, _StreamRecorder_recorder, "f"), controller);
        },
    });
}, _StreamRecorder_attachHandlers = function _StreamRecorder_attachHandlers(recorder, controller) {
    const onDataAvailableHandler = controller
        ? ({ data }) => {
            if (data && data.size > 0) {
                if (tslib.__classPrivateFieldGet(this, _StreamRecorder_chunks, "f").length === 0) {
                    tslib.__classPrivateFieldSet(this, _StreamRecorder_firstChunkTimestamp, Date.now(), "f");
                }
                tslib.__classPrivateFieldGet(this, _StreamRecorder_chunks, "f").push(data);
                controller.enqueue({ type: 'streamVideo', data });
            }
        }
        : ({ data }) => {
            tslib.__classPrivateFieldGet(this, _StreamRecorder_initialRecorder, "f").dispatchEvent(new MessageEvent('dataavailable', { data }));
        };
    recorder.ondataavailable = onDataAvailableHandler;
    const onSessionInfoHandler = controller
        ? (e) => {
            const { data } = e;
            controller.enqueue({ type: 'sessionInfo', data });
        }
        : (e) => {
            const { data } = e;
            tslib.__classPrivateFieldGet(this, _StreamRecorder_initialRecorder, "f").dispatchEvent(new MessageEvent('sessionInfo', { data }));
        };
    recorder.addEventListener('sessionInfo', onSessionInfoHandler);
    const onStreamStopHandler = controller
        ? () => {
            controller.enqueue({ type: 'streamStop' });
        }
        : () => {
            tslib.__classPrivateFieldGet(this, _StreamRecorder_initialRecorder, "f").dispatchEvent(new MessageEvent('streamStop'));
        };
    recorder.addEventListener('streamStop', onStreamStopHandler);
    const onCloseCodeHandler = controller
        ? (e) => {
            const { data } = e;
            controller.enqueue({ type: 'closeCode', data });
        }
        : (e) => {
            const { data } = e;
            tslib.__classPrivateFieldGet(this, _StreamRecorder_initialRecorder, "f").dispatchEvent(new MessageEvent('closeCode', { data }));
        };
    recorder.addEventListener('closeCode', onCloseCodeHandler);
    const onEndStreamHandler = controller
        ? () => {
            controller.close();
        }
        : () => {
            tslib.__classPrivateFieldGet(this, _StreamRecorder_initialRecorder, "f").dispatchEvent(new MessageEvent('endStream'));
        };
    recorder.addEventListener('endStream', onEndStreamHandler);
    tslib.__classPrivateFieldGet(this, _StreamRecorder_instances, "m", _StreamRecorder_setupCallbacks).call(this);
    tslib.__classPrivateFieldSet(this, _StreamRecorder_eventListeners, {
        endStream: onEndStreamHandler,
        closeCode: onCloseCodeHandler,
        streamStop: onStreamStopHandler,
        sessionInfo: onSessionInfoHandler,
        dataavailable: onDataAvailableHandler,
    }, "f");
}, _StreamRecorder_setupCallbacks = function _StreamRecorder_setupCallbacks() {
    tslib.__classPrivateFieldGet(this, _StreamRecorder_recorder, "f").onstart = () => {
        tslib.__classPrivateFieldSet(this, _StreamRecorder_recordingStarted, true, "f");
        tslib.__classPrivateFieldSet(this, _StreamRecorder_recorderStartTimestamp, Date.now(), "f");
    };
    tslib.__classPrivateFieldSet(this, _StreamRecorder_recorderStopped, new Promise((resolve) => {
        tslib.__classPrivateFieldGet(this, _StreamRecorder_recorder, "f").onstop = () => {
            tslib.__classPrivateFieldSet(this, _StreamRecorder_recorderEndTimestamp, Date.now(), "f");
            resolve();
        };
    }), "f");
    tslib.__classPrivateFieldGet(this, _StreamRecorder_recorder, "f").onerror = () => {
        this.stopRecording();
    };
}, _StreamRecorder_cleanUpEventListeners = function _StreamRecorder_cleanUpEventListeners() {
    const eventNames = Object.keys(tslib.__classPrivateFieldGet(this, _StreamRecorder_eventListeners, "f"));
    eventNames.forEach((name) => {
        tslib.__classPrivateFieldGet(this, _StreamRecorder_recorder, "f").removeEventListener(name, tslib.__classPrivateFieldGet(this, _StreamRecorder_eventListeners, "f")[name]);
    });
    tslib.__classPrivateFieldSet(this, _StreamRecorder_eventListeners, {}, "f");
};

const isServerSessionInformationEvent = (value) => {
    return !!value
        ?.ServerSessionInformationEvent;
};
const isConnectionTimeoutError = (error) => {
    const { message } = error;
    return message.includes(WEBSOCKET_CONNECTION_TIMEOUT_MESSAGE);
};
const isDisconnectionEvent = (value) => {
    return !!value
        ?.DisconnectionEvent;
};
const isValidationExceptionEvent = (value) => {
    return !!value
        ?.ValidationException;
};
const isInternalServerExceptionEvent = (value) => {
    return !!value
        ?.InternalServerException;
};
const isThrottlingExceptionEvent = (value) => {
    return !!value
        ?.ThrottlingException;
};
const isServiceQuotaExceededExceptionEvent = (value) => {
    return !!value
        ?.ServiceQuotaExceededException;
};
const isInvalidSignatureRegionException = (error) => {
    const { message, name } = error;
    return (name === 'InvalidSignatureException' && message.includes('valid region'));
};

const STATIC_VIDEO_CONSTRAINTS = {
    width: {
        min: 320,
        ideal: 640,
    },
    height: {
        min: 240,
        ideal: 480,
    },
    frameRate: { min: 15, ideal: 30, max: 60 },
    facingMode: 'user',
};

const CAMERA_ID_KEY = 'AmplifyLivenessCameraId';
const DEFAULT_FACE_FIT_TIMEOUT = 7000;
let responseStream;
// Helper function to get selected device info
const getSelectedDeviceInfo = (context) => {
    const selected = context.videoAssociatedParams?.selectableDevices?.find((d) => d.deviceId === context.videoAssociatedParams?.selectedDeviceId);
    return selected
        ? {
            deviceId: selected.deviceId,
            groupId: selected.groupId,
            label: selected.label ?? '',
        }
        : undefined;
};
const responseStreamActor = async (callback) => {
    try {
        const stream = await responseStream;
        for await (const event of stream) {
            if (isServerSessionInformationEvent(event)) {
                callback({
                    type: 'SET_SESSION_INFO',
                    data: {
                        serverSessionInformation: event.ServerSessionInformationEvent.SessionInformation,
                    },
                });
            }
            else if (isDisconnectionEvent(event)) {
                callback({ type: 'DISCONNECT_EVENT' });
            }
            else if (isValidationExceptionEvent(event)) {
                callback({
                    type: 'SERVER_ERROR',
                    data: { error: { ...event.ValidationException } },
                });
            }
            else if (isInternalServerExceptionEvent(event)) {
                callback({
                    type: 'SERVER_ERROR',
                    data: { error: { ...event.InternalServerException } },
                });
            }
            else if (isThrottlingExceptionEvent(event)) {
                callback({
                    type: 'SERVER_ERROR',
                    data: { error: { ...event.ThrottlingException } },
                });
            }
            else if (isServiceQuotaExceededExceptionEvent(event)) {
                callback({
                    type: 'SERVER_ERROR',
                    data: { error: { ...event.ServiceQuotaExceededException } },
                });
            }
        }
    }
    catch (error) {
        if (isInvalidSignatureRegionException(error)) {
            callback({
                type: 'SERVER_ERROR',
                data: {
                    error: new Error('Invalid region in FaceLivenessDetector or credentials are scoped to the wrong region.'),
                },
            });
        }
        else if (error instanceof Error) {
            callback({
                type: isConnectionTimeoutError(error)
                    ? 'CONNECTION_TIMEOUT'
                    : 'SERVER_ERROR',
                data: { error },
            });
        }
    }
};
function getLastSelectedCameraId() {
    return localStorage.getItem(CAMERA_ID_KEY);
}
function setLastSelectedCameraId(deviceId) {
    localStorage.setItem(CAMERA_ID_KEY, deviceId);
}
const livenessMachine = xstate.createMachine({
    id: 'livenessMachine',
    initial: 'initCamera',
    predictableActionArguments: true,
    context: {
        challengeId: uuid.v4(),
        errorMessage: undefined,
        maxFailedAttempts: 0,
        failedAttempts: 0,
        componentProps: undefined,
        parsedSessionInformation: undefined,
        serverSessionInformation: undefined,
        videoAssociatedParams: {
            videoConstraints: STATIC_VIDEO_CONSTRAINTS,
            selectableDevices: [],
        },
        ovalAssociatedParams: undefined,
        faceMatchAssociatedParams: {
            illuminationState: undefined,
            faceMatchState: undefined,
            /**
             * faceMatchPercentage is a starting point we set as a baseline
             * for what we want our progress bar to visually start at. This correlates
             * to the formula we use to calculate the faceMatchPercentage
             * in getFaceMatchStateInLivenessOval
             */
            faceMatchPercentage: 25,
            currentDetectedFace: undefined,
            startFace: undefined,
            endFace: undefined,
        },
        freshnessColorAssociatedParams: {
            freshnessColorEl: undefined,
            freshnessColors: [],
            freshnessColorsComplete: false,
        },
        colorSequenceDisplay: undefined,
        errorState: undefined,
        livenessStreamProvider: undefined,
        responseStreamActorRef: undefined,
        shouldDisconnect: false,
        faceMatchStateBeforeStart: undefined,
        isFaceFarEnoughBeforeRecording: undefined,
        isRecordingStopped: false,
    },
    on: {
        CANCEL: 'userCancel',
        TIMEOUT: {
            target: 'retryableTimeout',
            actions: 'updateErrorStateForTimeout',
        },
        SET_SESSION_INFO: { internal: true, actions: 'updateSessionInfo' },
        DISCONNECT_EVENT: { internal: true, actions: 'updateShouldDisconnect' },
        SET_DOM_AND_CAMERA_DETAILS: { actions: 'setDOMAndCameraDetails' },
        UPDATE_DEVICE_AND_STREAM: {
            actions: 'updateDeviceAndStream',
            target: 'start',
        },
        SERVER_ERROR: { target: 'error', actions: 'updateErrorStateForServer' },
        CONNECTION_TIMEOUT: {
            target: 'error',
            actions: 'updateErrorStateForConnectionTimeout',
        },
        RUNTIME_ERROR: { target: 'error', actions: 'updateErrorStateForRuntime' },
        MOBILE_LANDSCAPE_WARNING: {
            target: 'mobileLandscapeWarning',
            actions: 'updateErrorStateForServer',
        },
    },
    states: {
        initCamera: {
            initial: 'cameraCheck',
            on: {
                SET_DOM_AND_CAMERA_DETAILS: {
                    actions: 'setDOMAndCameraDetails',
                    target: '#livenessMachine.initWebsocket',
                },
            },
            states: {
                cameraCheck: {
                    entry: 'resetErrorState',
                    invoke: {
                        src: 'checkVirtualCameraAndGetStream',
                        onDone: {
                            target: 'waitForDOMAndCameraDetails',
                            actions: 'updateVideoMediaStream',
                        },
                        onError: { target: '#livenessMachine.permissionDenied' },
                    },
                },
                waitForDOMAndCameraDetails: {},
            },
        },
        initWebsocket: {
            entry: () => { },
            initial: 'initializeLivenessStream',
            states: {
                initializeLivenessStream: {
                    invoke: {
                        src: 'openLivenessStreamConnection',
                        onDone: {
                            target: 'waitForSessionInfo',
                            actions: [
                                'updateLivenessStreamProvider',
                                'spawnResponseStreamActor',
                            ],
                        },
                    },
                },
                waitForSessionInfo: {
                    entry: () => { },
                    after: {
                        0: {
                            target: '#livenessMachine.start',
                            cond: 'hasParsedSessionInfo',
                        },
                        100: { target: 'waitForSessionInfo' },
                    },
                },
            },
        },
        start: {
            entry: ['initializeFaceDetector', () => { }],
            always: [
                { target: 'detectFaceBeforeStart', cond: 'shouldSkipStartScreen' },
            ],
            on: { BEGIN: 'detectFaceBeforeStart' },
        },
        detectFaceBeforeStart: {
            invoke: {
                src: 'detectFace',
                onDone: {
                    target: 'checkFaceDetectedBeforeStart',
                    actions: 'updateFaceMatchBeforeStartDetails',
                },
            },
        },
        checkFaceDetectedBeforeStart: {
            after: {
                0: {
                    target: 'detectFaceDistanceBeforeRecording',
                    cond: 'hasSingleFaceBeforeStart',
                },
                100: { target: 'detectFaceBeforeStart' },
            },
        },
        detectFaceDistanceBeforeRecording: {
            invoke: {
                src: 'detectFaceDistance',
                onDone: {
                    target: 'checkFaceDistanceBeforeRecording',
                    actions: 'updateFaceDistanceBeforeRecording',
                },
            },
        },
        checkFaceDistanceBeforeRecording: {
            after: {
                0: {
                    target: 'recording',
                    cond: 'hasEnoughFaceDistanceBeforeRecording',
                },
                100: { target: 'detectFaceDistanceBeforeRecording' },
            },
        },
        recording: {
            entry: [
                'clearErrorState',
                'startRecording',
                'sendTimeoutAfterOvalDrawingDelay',
            ],
            initial: 'ovalDrawing',
            states: {
                ovalDrawing: {
                    invoke: {
                        src: 'detectInitialFaceAndDrawOval',
                        onDone: {
                            target: 'checkFaceDetected',
                            actions: [
                                'updateOvalAndFaceDetailsPostDraw',
                                'sendTimeoutAfterOvalMatchDelay',
                            ],
                        },
                        onError: {
                            target: '#livenessMachine.error',
                            actions: 'updateErrorStateForRuntime',
                        },
                    },
                },
                checkFaceDetected: {
                    after: {
                        0: { target: 'cancelOvalDrawingTimeout', cond: 'hasSingleFace' },
                        100: { target: 'ovalDrawing' },
                    },
                },
                cancelOvalDrawingTimeout: {
                    entry: [
                        'cancelOvalDrawingTimeout',
                        'sendTimeoutAfterRecordingDelay',
                    ],
                    after: { 0: { target: 'checkRecordingStarted' } },
                },
                checkRecordingStarted: {
                    entry: () => { },
                    after: {
                        0: {
                            target: 'ovalMatching',
                            cond: 'hasRecordingStarted',
                            actions: 'updateRecordingStartTimestamp',
                        },
                        100: { target: 'checkRecordingStarted' },
                    },
                },
                // Evaluates face match and moves to checkMatch
                // which continually checks for match until either timeout or face match
                ovalMatching: {
                    entry: 'cancelRecordingTimeout',
                    invoke: {
                        src: 'detectFaceAndMatchOval',
                        onDone: {
                            target: 'checkMatch',
                            actions: 'updateFaceDetailsPostMatch',
                        },
                    },
                },
                // If `hasFaceMatchedInOval` is true, then move to `delayBeforeFlash`, which pauses
                // for one second to show "Hold still" text before moving to `flashFreshnessColors`.
                // If not, move back to ovalMatching and re-evaluate match state
                checkMatch: {
                    entry: () => { },
                    after: {
                        0: {
                            target: 'handleChallenge',
                            cond: 'hasFaceMatchedInOval',
                            actions: [
                                'setFaceMatchTimeAndStartFace',
                                'updateEndFaceMatch',
                                'setColorDisplay',
                                'cancelOvalMatchTimeout',
                                'cancelOvalDrawingTimeout',
                            ],
                        },
                        1: { target: 'ovalMatching' },
                    },
                },
                handleChallenge: {
                    entry: () => { },
                    always: [
                        {
                            target: 'delayBeforeFlash',
                            cond: 'isFaceMovementAndLightChallenge',
                        },
                        { target: 'success', cond: 'isFaceMovementChallenge' },
                    ],
                },
                delayBeforeFlash: {
                    entry: () => { },
                    after: { 1000: 'flashFreshnessColors' },
                },
                flashFreshnessColors: {
                    invoke: {
                        src: 'flashColors',
                        onDone: [
                            { target: 'success', cond: 'hasFreshnessColorShown' },
                            {
                                target: 'flashFreshnessColors',
                                actions: 'updateFreshnessDetails',
                            },
                        ],
                    },
                },
                success: { entry: 'stopRecording', type: 'final' },
            },
            onDone: 'uploading',
        },
        uploading: {
            initial: 'pending',
            states: {
                pending: {
                    entry: ['pauseVideoStream'],
                    invoke: {
                        src: 'stopVideo',
                        onDone: 'waitForDisconnectEvent',
                        onError: {
                            target: '#livenessMachine.error',
                            actions: 'updateErrorStateForRuntime',
                        },
                    },
                },
                waitForDisconnectEvent: {
                    entry: () => { },
                    after: {
                        0: { cond: 'getShouldDisconnect', target: 'getLivenessResult' },
                        100: { target: 'waitForDisconnectEvent' },
                    },
                },
                getLivenessResult: {
                    entry: ['freezeStream'],
                    invoke: {
                        src: 'getLiveness',
                        onError: {
                            target: '#livenessMachine.error',
                            actions: 'updateErrorStateForServer',
                        },
                    },
                },
            },
        },
        retryableTimeout: {
            entry: 'updateFailedAttempts',
            always: [
                { target: 'timeout', cond: 'shouldTimeoutOnFailedAttempts' },
                { target: 'start' },
            ],
        },
        permissionDenied: {
            entry: 'callUserPermissionDeniedCallback',
            on: { RETRY_CAMERA_CHECK: 'initCamera' },
        },
        mobileLandscapeWarning: {
            entry: 'callMobileLandscapeWarningCallback',
            always: { target: 'error' },
        },
        timeout: {
            entry: ['cleanUpResources', 'callUserTimeoutCallback', 'freezeStream'],
        },
        error: {
            entry: [
                'cleanUpResources',
                'callErrorCallback',
                'cancelOvalDrawingTimeout',
                'cancelOvalMatchTimeout',
                'cancelRecordingTimeout',
                'freezeStream',
            ],
        },
        userCancel: {
            entry: ['cleanUpResources', 'callUserCancelCallback', 'resetContext'],
            always: { target: 'initCamera' },
        },
    },
}, {
    actions: {
        spawnResponseStreamActor: xstate.assign({
            responseStreamActorRef: () => xstate.spawn(responseStreamActor),
        }),
        updateFailedAttempts: xstate.assign({
            failedAttempts: (context) => context.failedAttempts + 1,
        }),
        updateVideoMediaStream: xstate.assign({
            videoAssociatedParams: (context, event) => ({
                ...context.videoAssociatedParams,
                videoMediaStream: event.data
                    ?.stream,
                selectedDeviceId: event.data
                    ?.selectedDeviceId,
                selectableDevices: event.data
                    ?.selectableDevices,
            }),
        }),
        initializeFaceDetector: xstate.assign({
            ovalAssociatedParams: (context) => {
                const { componentProps } = context;
                const { faceModelUrl, binaryPath } = componentProps.config;
                const faceDetector = new BlazeFaceFaceDetection(binaryPath, faceModelUrl);
                faceDetector.triggerModelLoading();
                return { ...context.ovalAssociatedParams, faceDetector };
            },
        }),
        updateLivenessStreamProvider: xstate.assign({
            livenessStreamProvider: (context, event) => event.data?.livenessStreamProvider,
        }),
        setDOMAndCameraDetails: xstate.assign({
            videoAssociatedParams: (context, event) => ({
                ...context.videoAssociatedParams,
                videoEl: event.data?.videoEl,
                canvasEl: event.data?.canvasEl,
                isMobile: event.data?.isMobile,
            }),
            freshnessColorAssociatedParams: (context, event) => ({
                ...context.freshnessColorAssociatedParams,
                freshnessColorEl: event.data
                    ?.freshnessColorEl,
            }),
        }),
        updateDeviceAndStream: xstate.assign({
            videoAssociatedParams: (context, event) => {
                setLastSelectedCameraId(event.data?.newDeviceId);
                context.livenessStreamProvider?.setNewVideoStream(event.data?.newStream);
                return {
                    ...context.videoAssociatedParams,
                    selectedDeviceId: event.data
                        ?.newDeviceId,
                    videoMediaStream: event.data
                        ?.newStream,
                };
            },
        }),
        updateRecordingStartTimestamp: xstate.assign({
            videoAssociatedParams: (context) => {
                const { challengeId, ovalAssociatedParams, videoAssociatedParams, livenessStreamProvider, parsedSessionInformation, } = context;
                const { videoMediaStream } = videoAssociatedParams;
                const recordingStartedTimestamp = livenessStreamProvider.getRecordingStartTimestamp();
                context.livenessStreamProvider.dispatchStreamEvent({
                    type: 'sessionInfo',
                    data: createSessionStartEvent({
                        parsedSessionInformation: parsedSessionInformation,
                        ...getTrackDimensions(videoMediaStream),
                        challengeId: challengeId,
                        ovalAssociatedParams: ovalAssociatedParams,
                        recordingStartedTimestamp,
                    }),
                });
                return {
                    ...context.videoAssociatedParams,
                    recordingStartedTimestamp,
                };
            },
        }),
        startRecording: xstate.assign({
            videoAssociatedParams: (context) => {
                if (context.livenessStreamProvider &&
                    !context.livenessStreamProvider.isRecording()) {
                    context.livenessStreamProvider.startRecording();
                }
                return { ...context.videoAssociatedParams };
            },
        }),
        stopRecording: () => { },
        updateFaceMatchBeforeStartDetails: xstate.assign({
            faceMatchStateBeforeStart: (_, event) => event.data.faceMatchState,
        }),
        updateFaceDistanceBeforeRecording: xstate.assign({
            isFaceFarEnoughBeforeRecording: (_, event) => !!event.data.isFaceFarEnoughBeforeRecording,
        }),
        updateFaceDistanceWhileLoading: xstate.assign({
            isFaceFarEnoughBeforeRecording: (_, event) => !!event.data.isFaceFarEnoughBeforeRecording,
            errorState: (_, event) => event.data?.error,
        }),
        updateOvalAndFaceDetailsPostDraw: xstate.assign({
            ovalAssociatedParams: (context, event) => ({
                ...context.ovalAssociatedParams,
                initialFace: event.data
                    .initialFace,
                ovalDetails: event.data
                    .ovalDetails,
                scaleFactor: event.data
                    .scaleFactor,
            }),
            faceMatchAssociatedParams: (context, event) => ({
                ...context.faceMatchAssociatedParams,
                faceMatchState: event.data
                    .faceMatchState,
                illuminationState: event.data
                    .illuminationState,
            }),
        }),
        updateFaceDetailsPostMatch: xstate.assign({
            faceMatchAssociatedParams: (context, event) => ({
                ...context.faceMatchAssociatedParams,
                faceMatchState: event.data
                    .faceMatchState,
                faceMatchPercentage: event.data
                    .faceMatchPercentage,
                illuminationState: event.data
                    .illuminationState,
                currentDetectedFace: event.data
                    .detectedFace,
            }),
        }),
        updateEndFaceMatch: xstate.assign({
            faceMatchAssociatedParams: (context) => ({
                ...context.faceMatchAssociatedParams,
                endFace: context.faceMatchAssociatedParams.currentDetectedFace,
            }),
        }),
        setFaceMatchTimeAndStartFace: xstate.assign({
            faceMatchAssociatedParams: (context) => {
                return {
                    ...context.faceMatchAssociatedParams,
                    startFace: context.faceMatchAssociatedParams.startFace === undefined
                        ? context.faceMatchAssociatedParams.currentDetectedFace
                        : context.faceMatchAssociatedParams.startFace,
                };
            },
        }),
        resetErrorState: xstate.assign({ errorState: (_) => undefined }),
        updateErrorStateForConnectionTimeout: xstate.assign({
            errorState: (_) => LivenessErrorState.CONNECTION_TIMEOUT,
        }),
        updateErrorStateForTimeout: xstate.assign({
            errorState: (_, event) => event.data?.errorState || LivenessErrorState.TIMEOUT,
            errorMessage: (_, event) => event.data?.message,
        }),
        updateErrorStateForRuntime: xstate.assign({
            errorState: (_, event) => event.data?.errorState ||
                LivenessErrorState.RUNTIME_ERROR,
        }),
        updateErrorStateForServer: xstate.assign({
            errorState: (_) => LivenessErrorState.SERVER_ERROR,
        }),
        clearErrorState: xstate.assign({ errorState: (_) => undefined }),
        updateSessionInfo: xstate.assign({
            parsedSessionInformation: (_, event) => {
                const { serverSessionInformation } = event.data;
                return createSessionInfoFromServerSessionInformation(serverSessionInformation);
            },
        }),
        updateShouldDisconnect: xstate.assign({ shouldDisconnect: () => true }),
        updateFreshnessDetails: xstate.assign({
            freshnessColorAssociatedParams: (context, event) => {
                return {
                    ...context.freshnessColorAssociatedParams,
                    freshnessColorsComplete: event.data
                        .freshnessColorsComplete,
                };
            },
        }),
        setColorDisplay: xstate.assign({
            colorSequenceDisplay: ({ parsedSessionInformation }) => new ColorSequenceDisplay(getColorsSequencesFromSessionInformation(parsedSessionInformation)),
        }),
        // timeouts
        sendTimeoutAfterOvalDrawingDelay: xstate.actions.send({
            type: 'RUNTIME_ERROR',
            data: { message: 'Client failed to draw oval.' },
        }, { delay: 5000, id: 'ovalDrawingTimeout' }),
        cancelOvalDrawingTimeout: xstate.actions.cancel('ovalDrawingTimeout'),
        sendTimeoutAfterRecordingDelay: xstate.actions.send({
            type: 'RUNTIME_ERROR',
            data: { message: 'Client failed to start recording.' },
        }, { delay: 5000, id: 'recordingTimeout' }),
        cancelRecordingTimeout: xstate.actions.cancel('recordingTimeout'),
        sendTimeoutAfterOvalMatchDelay: xstate.actions.send({
            type: 'TIMEOUT',
            data: { message: 'Client timed out waiting for face to match oval.' },
        }, {
            delay: (context) => {
                return (context.parsedSessionInformation.Challenge.ChallengeConfig
                    ?.OvalFitTimeout ?? DEFAULT_FACE_FIT_TIMEOUT);
            },
            id: 'ovalMatchTimeout',
        }),
        cancelOvalMatchTimeout: xstate.actions.cancel('ovalMatchTimeout'),
        // callbacks
        callUserPermissionDeniedCallback: xstate.assign({
            errorState: (context, event) => {
                let errorState;
                if (event.data.message.includes('15 fps')) {
                    errorState = LivenessErrorState.CAMERA_FRAMERATE_ERROR;
                }
                else if (event.data.message.includes(LivenessErrorState.DEFAULT_CAMERA_NOT_FOUND_ERROR)) {
                    errorState = LivenessErrorState.DEFAULT_CAMERA_NOT_FOUND_ERROR;
                }
                else {
                    errorState = LivenessErrorState.CAMERA_ACCESS_ERROR;
                }
                const errorMessage = event.data.message || event.data.Message;
                const error = new Error(errorMessage);
                const livenessError = {
                    state: errorState,
                    error: error,
                };
                context.componentProps.onError?.(livenessError, getSelectedDeviceInfo(context));
                return errorState;
            },
        }),
        callMobileLandscapeWarningCallback: xstate.assign({
            errorState: () => LivenessErrorState.MOBILE_LANDSCAPE_ERROR,
        }),
        getSelectedDeviceInfo: (context) => getSelectedDeviceInfo(context),
        callUserCancelCallback: (context) => {
            const { onUserCancel } = context.componentProps ?? {};
            if (!onUserCancel) {
                return;
            }
            try {
                onUserCancel();
            }
            catch (callbackError) {
                // eslint-disable-next-line no-console
                console.error('Error in onUserCancel callback:', callbackError);
            }
        },
        callUserTimeoutCallback: (context) => {
            const error = new Error(context.errorMessage ?? 'Client Timeout');
            error.name = context.errorState;
            const livenessError = {
                state: context.errorState,
                error: error,
            };
            context.componentProps.onError?.(livenessError, getSelectedDeviceInfo(context));
        },
        callErrorCallback: (context, event) => {
            const livenessError = {
                state: context.errorState,
                error: event.data?.error || event.data,
            };
            context.componentProps.onError?.(livenessError, getSelectedDeviceInfo(context));
        },
        cleanUpResources: (context) => {
            const { freshnessColorEl } = context.freshnessColorAssociatedParams;
            if (freshnessColorEl) {
                freshnessColorEl.style.display = 'none';
            }
            let closeCode = WS_CLOSURE_CODE.DEFAULT_ERROR_CODE;
            if (context.errorState === LivenessErrorState.TIMEOUT) {
                closeCode = WS_CLOSURE_CODE.FACE_FIT_TIMEOUT;
            }
            else if (context.errorState === LivenessErrorState.RUNTIME_ERROR) {
                closeCode = WS_CLOSURE_CODE.RUNTIME_ERROR;
            }
            else if (context.errorState === LivenessErrorState.FACE_DISTANCE_ERROR ||
                context.errorState === LivenessErrorState.MULTIPLE_FACES_ERROR) {
                closeCode = WS_CLOSURE_CODE.USER_ERROR_DURING_CONNECTION;
            }
            else if (context.errorState === undefined) {
                closeCode = WS_CLOSURE_CODE.USER_CANCEL;
            }
            context.livenessStreamProvider?.stopRecording().then(() => {
                context.livenessStreamProvider?.dispatchStreamEvent({
                    type: 'closeCode',
                    data: { closeCode },
                });
            });
        },
        freezeStream: (context) => {
            const { videoMediaStream, videoEl } = context.videoAssociatedParams;
            context.isRecordingStopped = true;
            videoEl?.pause();
            videoMediaStream?.getTracks().forEach(function (track) {
                track.stop();
            });
        },
        pauseVideoStream: (context) => {
            const { videoEl } = context.videoAssociatedParams;
            context.isRecordingStopped = true;
            videoEl.pause();
        },
        resetContext: xstate.assign({
            challengeId: uuid.v4(),
            maxFailedAttempts: 0,
            failedAttempts: 0,
            componentProps: (context) => context.componentProps,
            parsedSessionInformation: (_) => undefined,
            videoAssociatedParams: (_) => {
                return { videoConstraints: STATIC_VIDEO_CONSTRAINTS };
            },
            ovalAssociatedParams: (_) => undefined,
            errorState: (_) => undefined,
            livenessStreamProvider: (_) => undefined,
            responseStreamActorRef: (_) => undefined,
            shouldDisconnect: false,
            faceMatchStateBeforeStart: (_) => undefined,
            isFaceFarEnoughBeforeRecording: (_) => undefined,
            isRecordingStopped: false,
        }),
    },
    guards: {
        shouldTimeoutOnFailedAttempts: (context) => context.failedAttempts >= context.maxFailedAttempts,
        hasFaceMatchedInOval: (context) => {
            return (context.faceMatchAssociatedParams.faceMatchState ===
                FaceMatchState.MATCHED);
        },
        hasSingleFace: (context) => {
            return (context.faceMatchAssociatedParams.faceMatchState ===
                FaceMatchState.FACE_IDENTIFIED);
        },
        hasSingleFaceBeforeStart: (context) => {
            return (context.faceMatchStateBeforeStart === FaceMatchState.FACE_IDENTIFIED);
        },
        hasEnoughFaceDistanceBeforeRecording: (context) => {
            return context.isFaceFarEnoughBeforeRecording;
        },
        hasNotEnoughFaceDistanceBeforeRecording: (context) => {
            return !context.isFaceFarEnoughBeforeRecording;
        },
        hasFreshnessColorShown: (context) => {
            return context.freshnessColorAssociatedParams.freshnessColorsComplete;
        },
        hasParsedSessionInfo: (context) => {
            return context.parsedSessionInformation !== undefined;
        },
        hasDOMAndCameraDetails: (context) => {
            return (context.videoAssociatedParams.videoEl !== undefined &&
                context.videoAssociatedParams.canvasEl !== undefined &&
                context.freshnessColorAssociatedParams.freshnessColorEl !== undefined);
        },
        isFaceMovementChallenge: (context) => {
            return (context.parsedSessionInformation?.Challenge?.Name ===
                FACE_MOVEMENT_CHALLENGE.type);
        },
        isFaceMovementAndLightChallenge: (context) => {
            return (context.parsedSessionInformation?.Challenge?.Name ===
                FACE_MOVEMENT_AND_LIGHT_CHALLENGE.type);
        },
        getShouldDisconnect: (context) => {
            return !!context.shouldDisconnect;
        },
        hasRecordingStarted: (context) => {
            return context.livenessStreamProvider.hasRecordingStarted();
        },
        shouldSkipStartScreen: (context) => {
            return !!context.componentProps?.disableStartScreen;
        },
    },
    services: {
        async checkVirtualCameraAndGetStream(context) {
            const { videoConstraints } = context.videoAssociatedParams;
            const { componentProps } = context;
            let targetDeviceId;
            let cameraNotFound = false;
            if (componentProps?.config?.deviceId) {
                targetDeviceId = componentProps.config.deviceId;
            }
            else {
                targetDeviceId = getLastSelectedCameraId() ?? undefined;
            }
            const initialStream = await navigator.mediaDevices
                .getUserMedia({
                video: {
                    ...videoConstraints,
                    ...(targetDeviceId
                        ? { deviceId: { exact: targetDeviceId } }
                        : {}),
                },
                audio: false,
            })
                .catch((error) => {
                if (error instanceof DOMException &&
                    (error.name === 'NotFoundError' ||
                        error.name === 'OverconstrainedError')) {
                    // Mark camera as not found when a specific target device (either provided via props
                    // or previously selected/saved as default) cannot be accessed.
                    if (targetDeviceId && !cameraNotFound) {
                        cameraNotFound = true;
                    }
                    return navigator.mediaDevices.getUserMedia({
                        video: {
                            ...videoConstraints,
                        },
                        audio: false,
                    });
                }
                throw error;
            });
            const devices = await navigator.mediaDevices.enumerateDevices();
            const realVideoDevices = devices
                .filter((device) => device.kind === 'videoinput')
                .filter((device) => !isCameraDeviceVirtual(device));
            if (!realVideoDevices.length) {
                throw new Error('No real video devices found');
            }
            // Ensure that at least one of the cameras is capable of at least 15 fps
            const tracksWithMoreThan15Fps = initialStream
                .getTracks()
                .filter((track) => {
                const settings = track.getSettings();
                return (settings.frameRate ?? 0) >= 15;
            });
            if (tracksWithMoreThan15Fps.length < 1) {
                throw new Error('No camera found with more than 15 fps');
            }
            // If the initial stream is of real camera, use it otherwise use the first real camera
            const initialStreamDeviceId = tracksWithMoreThan15Fps[0].getSettings().deviceId;
            const isInitialStreamFromRealDevice = realVideoDevices.some((device) => device.deviceId === initialStreamDeviceId);
            const deviceId = isInitialStreamFromRealDevice
                ? initialStreamDeviceId
                : realVideoDevices[0].deviceId;
            let realVideoDeviceStream = initialStream;
            if (!isInitialStreamFromRealDevice) {
                realVideoDeviceStream = await navigator.mediaDevices.getUserMedia({
                    video: { ...videoConstraints, deviceId: { exact: deviceId } },
                    audio: false,
                });
            }
            setLastSelectedCameraId(deviceId);
            const result = {
                stream: realVideoDeviceStream,
                selectedDeviceId: initialStreamDeviceId,
                selectableDevices: realVideoDevices,
            };
            // If a specific camera was requested but not found, trigger a specific error
            if (cameraNotFound) {
                throw new Error(LivenessErrorState.DEFAULT_CAMERA_NOT_FOUND_ERROR);
            }
            return result;
        },
        // eslint-disable-next-line @typescript-eslint/require-await
        async openLivenessStreamConnection(context) {
            const { config, disableStartScreen } = context.componentProps;
            const { credentialProvider, endpointOverride, systemClockOffset } = config;
            const { videoHeight, videoWidth } = context.videoAssociatedParams.videoEl;
            const livenessStreamProvider = new StreamRecorder(context.videoAssociatedParams.videoMediaStream);
            const requestStream = createRequestStreamGenerator(livenessStreamProvider.getVideoStream()).getRequestStream();
            const { getResponseStream } = await createStreamingClient({
                credentialsProvider: credentialProvider,
                endpointOverride,
                region: context.componentProps.region,
                attemptCount: TelemetryReporter.getAttemptCountAndUpdateTimestamp(),
                preCheckViewEnabled: !disableStartScreen,
                systemClockOffset,
            });
            responseStream = getResponseStream({
                requestStream,
                sessionId: context.componentProps.sessionId,
                videoHeight: videoHeight.toString(),
                videoWidth: videoWidth.toString(),
            });
            return { livenessStreamProvider };
        },
        async detectFace(context) {
            const { videoEl } = context.videoAssociatedParams;
            const { faceDetector } = context.ovalAssociatedParams;
            // initialize models
            try {
                await faceDetector.modelLoadingPromise;
            }
            catch (err) {
                // eslint-disable-next-line no-console
                console.log({ err });
            }
            // detect face
            const faceMatchState = await getFaceMatchState(faceDetector, videoEl);
            return { faceMatchState };
        },
        async detectFaceDistance(context) {
            const { parsedSessionInformation, isFaceFarEnoughBeforeRecording: faceDistanceCheckBeforeRecording, } = context;
            const { videoEl, videoMediaStream } = context.videoAssociatedParams;
            const { faceDetector } = context.ovalAssociatedParams;
            const { width, height } = videoMediaStream
                .getTracks()[0]
                .getSettings();
            const challengeConfig = parsedSessionInformation.Challenge.ChallengeConfig;
            const ovalDetails = getStaticLivenessOvalDetails({
                width: width,
                height: height,
                ovalHeightWidthRatio: challengeConfig.OvalHeightWidthRatio,
            });
            const { isDistanceBelowThreshold: isFaceFarEnoughBeforeRecording } = await isFaceDistanceBelowThreshold({
                parsedSessionInformation: parsedSessionInformation,
                faceDetector: faceDetector,
                videoEl: videoEl,
                ovalDetails,
                reduceThreshold: faceDistanceCheckBeforeRecording, // if this is the second face distance check reduce the threshold
            });
            return { isFaceFarEnoughBeforeRecording };
        },
        async detectInitialFaceAndDrawOval(context) {
            const { parsedSessionInformation } = context;
            const { videoEl, canvasEl, isMobile } = context.videoAssociatedParams;
            const { faceDetector } = context.ovalAssociatedParams;
            // initialize models
            try {
                await faceDetector.modelLoadingPromise;
            }
            catch (err) {
                // eslint-disable-next-line no-console
                console.log({ err });
            }
            // detect face
            const detectedFaces = await faceDetector.detectFaces(videoEl);
            let initialFace;
            let faceMatchState;
            let illuminationState;
            switch (detectedFaces.length) {
                case 0: {
                    // no face detected;
                    faceMatchState = FaceMatchState.CANT_IDENTIFY;
                    illuminationState = estimateIllumination(videoEl);
                    break;
                }
                case 1: {
                    //exactly one face detected;
                    faceMatchState = FaceMatchState.FACE_IDENTIFIED;
                    initialFace = detectedFaces[0];
                    break;
                }
                default: {
                    //more than one face detected ;
                    faceMatchState = FaceMatchState.TOO_MANY;
                    break;
                }
            }
            if (!initialFace) {
                return { faceMatchState, illuminationState };
            }
            // Get width/height of video element so we can compute scaleFactor
            // and set canvas width/height.
            const { width: videoScaledWidth, height: videoScaledHeight } = videoEl.getBoundingClientRect();
            if (isMobile) {
                canvasEl.width = window.innerWidth;
                canvasEl.height = window.innerHeight;
            }
            else {
                canvasEl.width = videoScaledWidth;
                canvasEl.height = videoScaledHeight;
            }
            // Compute scaleFactor which is how much our video element is scaled
            // vs the intrinsic video resolution
            const scaleFactor = videoScaledWidth / videoEl.videoWidth;
            // generate oval details from initialFace and video dimensions
            const ovalDetails = getOvalDetailsFromSessionInformation({
                parsedSessionInformation: parsedSessionInformation,
                videoWidth: videoEl.width,
            });
            const challengeConfig = parsedSessionInformation.Challenge.ChallengeConfig;
            // renormalize initial face
            const renormalizedFace = generateBboxFromLandmarks({
                ovalHeightWidthRatio: challengeConfig.OvalHeightWidthRatio,
                face: initialFace,
                oval: ovalDetails,
                frameHeight: videoEl.videoHeight,
            });
            initialFace.top = renormalizedFace.top;
            initialFace.left = renormalizedFace.left;
            initialFace.height = renormalizedFace.bottom - renormalizedFace.top;
            initialFace.width = renormalizedFace.right - renormalizedFace.left;
            // Draw oval in canvas using ovalDetails and scaleFactor
            drawLivenessOvalInCanvas({
                canvas: canvasEl,
                oval: ovalDetails,
                scaleFactor,
                videoEl: videoEl,
            });
            return { faceMatchState, ovalDetails, scaleFactor, initialFace };
        },
        async detectFaceAndMatchOval(context) {
            const { parsedSessionInformation } = context;
            const { videoEl } = context.videoAssociatedParams;
            const { faceDetector, ovalDetails, initialFace } = context.ovalAssociatedParams;
            // detect face
            const detectedFaces = await faceDetector.detectFaces(videoEl);
            let faceMatchState;
            let faceMatchPercentage = 0;
            let detectedFace;
            let illuminationState;
            const challengeConfig = parsedSessionInformation.Challenge.ChallengeConfig;
            const initialFaceBoundingBox = generateBboxFromLandmarks({
                ovalHeightWidthRatio: challengeConfig.OvalHeightWidthRatio,
                face: initialFace,
                oval: ovalDetails,
                frameHeight: videoEl.videoHeight,
            });
            const { ovalBoundingBox } = getOvalBoundingBox(ovalDetails);
            const initialFaceIntersection = getIntersectionOverUnion(initialFaceBoundingBox, ovalBoundingBox);
            switch (detectedFaces.length) {
                case 0: {
                    //no face detected;
                    faceMatchState = FaceMatchState.CANT_IDENTIFY;
                    illuminationState = estimateIllumination(videoEl);
                    break;
                }
                case 1: {
                    //exactly one face detected, match face with oval;
                    detectedFace = detectedFaces[0];
                    const { faceMatchState: faceMatchStateInLivenessOval, faceMatchPercentage: faceMatchPercentageInLivenessOval, } = getFaceMatchStateInLivenessOval({
                        face: detectedFace,
                        ovalDetails: ovalDetails,
                        initialFaceIntersection,
                        parsedSessionInformation: parsedSessionInformation,
                        frameHeight: videoEl.videoHeight,
                    });
                    faceMatchState = faceMatchStateInLivenessOval;
                    faceMatchPercentage = faceMatchPercentageInLivenessOval;
                    break;
                }
                default: {
                    //more than one face detected ;
                    faceMatchState = FaceMatchState.TOO_MANY;
                    break;
                }
            }
            return {
                faceMatchState,
                faceMatchPercentage,
                illuminationState,
                detectedFace,
            };
        },
        async flashColors({ challengeId, colorSequenceDisplay, freshnessColorAssociatedParams, livenessStreamProvider, ovalAssociatedParams, videoAssociatedParams, }) {
            const { freshnessColorsComplete, freshnessColorEl } = freshnessColorAssociatedParams;
            if (freshnessColorsComplete) {
                return;
            }
            const { ovalDetails, scaleFactor } = ovalAssociatedParams;
            const { videoEl } = videoAssociatedParams;
            const completed = await colorSequenceDisplay.startSequences({
                onSequenceColorChange: ({ sequenceColor, prevSequenceColor, heightFraction, }) => {
                    fillOverlayCanvasFractional({
                        heightFraction,
                        overlayCanvas: freshnessColorEl,
                        ovalDetails: ovalDetails,
                        nextColor: sequenceColor,
                        prevColor: prevSequenceColor,
                        scaleFactor: scaleFactor,
                        videoEl: videoEl,
                    });
                },
                onSequenceStart: () => {
                    freshnessColorEl.style.display = 'block';
                },
                onSequencesComplete: () => {
                    freshnessColorEl.style.display = 'none';
                },
                onSequenceChange: (params) => {
                    livenessStreamProvider.dispatchStreamEvent({
                        type: 'sessionInfo',
                        data: createColorDisplayEvent({
                            ...params,
                            challengeId: challengeId,
                        }),
                    });
                },
            });
            return { freshnessColorsComplete: completed };
        },
        async stopVideo(context) {
            const { challengeId, parsedSessionInformation, faceMatchAssociatedParams, ovalAssociatedParams, livenessStreamProvider, videoAssociatedParams, } = context;
            const { videoMediaStream } = videoAssociatedParams;
            // if not awaited, `getRecordingEndTimestamp` will throw
            await livenessStreamProvider.stopRecording();
            if (livenessStreamProvider.getChunksLength() === 0) {
                throw new Error('Video chunks not recorded successfully.');
            }
            livenessStreamProvider.dispatchStreamEvent({
                type: 'sessionInfo',
                data: createSessionEndEvent({
                    ...getTrackDimensions(videoMediaStream),
                    parsedSessionInformation: parsedSessionInformation,
                    challengeId: challengeId,
                    faceMatchAssociatedParams: faceMatchAssociatedParams,
                    ovalAssociatedParams: ovalAssociatedParams,
                    recordingEndedTimestamp: livenessStreamProvider.getRecordingEndedTimestamp(),
                }),
            });
            livenessStreamProvider.dispatchStreamEvent({ type: 'streamStop' });
        },
        async getLiveness(context) {
            const { onAnalysisComplete } = context.componentProps ?? {};
            if (!onAnalysisComplete) {
                return;
            }
            try {
                const deviceInfo = getSelectedDeviceInfo(context);
                await onAnalysisComplete(deviceInfo);
            }
            catch (callbackError) {
                // eslint-disable-next-line no-console
                console.error('Error in onAnalysisComplete callback:', callbackError);
                // Rethrow to allow the state machine to handle the error
                throw callbackError;
            }
        },
    },
});

const FaceLivenessDetectorContext = React__namespace["default"].createContext(null);
function FaceLivenessDetectorProvider({ children, ...props }) {
    return (React__namespace["default"].createElement(FaceLivenessDetectorContext.Provider, { value: props }, children));
}
function useFaceLivenessDetector() {
    const props = React__namespace["default"].useContext(FaceLivenessDetectorContext);
    if (props === null) {
        throw new Error('useFaceLivenessDetector must be used within a FaceLivenessDetectorProvider');
    }
    return props;
}

// TODO: Add type annotations. Currently typing the actors returned from Xstate is difficult
// because the interpreter and state can not be used to form a type.
// eslint-disable-next-line @typescript-eslint/explicit-module-boundary-types
function useLivenessActor() {
    const { service } = useFaceLivenessDetector();
    const actor = react.useActor(service);
    return actor;
}

function createLivenessSelector(selector) {
    return selector;
}
function useLivenessSelector(selector) {
    const { service } = useFaceLivenessDetector();
    return react.useSelector(service, selector);
}

function useMediaStreamInVideo(stream) {
    const height = STATIC_VIDEO_CONSTRAINTS.height.ideal;
    const width = STATIC_VIDEO_CONSTRAINTS.width.ideal;
    const videoRef = React.useRef(null);
    const [videoHeight, setVideoHeight] = React.useState(height);
    const [videoWidth, setVideoWidth] = React.useState(width);
    React.useEffect(() => {
        if (stream) {
            if (ui.isObject(videoRef.current)) {
                videoRef.current.srcObject = stream;
            }
            const settings = stream.getTracks()?.[0]?.getSettings();
            if (settings) {
                setVideoHeight(settings.height);
                setVideoWidth(settings.width);
            }
        }
        return () => {
            if (stream) {
                stream.getTracks().forEach((track) => {
                    stream.removeTrack(track);
                    track.stop();
                });
            }
        };
    }, [stream]);
    return {
        videoRef,
        videoHeight,
        videoWidth,
    };
}

var LivenessClassNames;
(function (LivenessClassNames) {
    LivenessClassNames["CameraModule"] = "amplify-liveness-camera-module";
    LivenessClassNames["CancelContainer"] = "amplify-liveness-cancel-container";
    LivenessClassNames["CancelButton"] = "amplify-liveness-cancel-button";
    LivenessClassNames["CenteredLoader"] = "amplify-liveness-centered-loader";
    LivenessClassNames["ConnectingLoader"] = "amplify-liveness-connecting-loader";
    LivenessClassNames["CountdownContainer"] = "amplify-liveness-countdown-container";
    LivenessClassNames["DescriptionBullet"] = "amplify-liveness-description-bullet";
    LivenessClassNames["DescriptionBulletIndex"] = "amplify-liveness-description-bullet__index";
    LivenessClassNames["DescriptionBulletIndexText"] = "amplify-liveness-description-bullet__index__text";
    LivenessClassNames["DescriptionBulletMessage"] = "amplify-liveness-description-bullet__message";
    LivenessClassNames["ErrorModal"] = "amplify-liveness-error-modal";
    LivenessClassNames["ErrorModalHeading"] = "amplify-liveness-error-modal__heading";
    LivenessClassNames["FadeOut"] = "amplify-liveness-fade-out";
    LivenessClassNames["FreshnessCanvas"] = "amplify-liveness-freshness-canvas";
    LivenessClassNames["InstructionList"] = "amplify-liveness-instruction-list";
    LivenessClassNames["InstructionOverlay"] = "amplify-liveness-instruction-overlay";
    LivenessClassNames["Hint"] = "amplify-liveness-hint";
    LivenessClassNames["HintText"] = "amplify-liveness-hint__text";
    LivenessClassNames["LandscapeErrorModal"] = "amplify-liveness-landscape-error-modal";
    LivenessClassNames["LandscapeErrorModalButton"] = "amplify-liveness-landscape-error-modal__button";
    LivenessClassNames["LandscapeErrorModalHeader"] = "amplify-liveness-landscape-error-modal__header";
    LivenessClassNames["Loader"] = "amplify-liveness-loader";
    LivenessClassNames["MatchIndicator"] = "amplify-liveness-match-indicator";
    LivenessClassNames["OvalCanvas"] = "amplify-liveness-oval-canvas";
    LivenessClassNames["OpaqueOverlay"] = "amplify-liveness-overlay-opaque";
    LivenessClassNames["Overlay"] = "amplify-liveness-overlay";
    LivenessClassNames["Popover"] = "amplify-liveness-popover";
    LivenessClassNames["PopoverContainer"] = "amplify-liveness-popover__container";
    LivenessClassNames["PopoverAnchor"] = "amplify-liveness-popover__anchor";
    LivenessClassNames["PopoverAnchorSecondary"] = "amplify-liveness-popover__anchor-secondary";
    LivenessClassNames["RecordingIconContainer"] = "amplify-liveness-recording-icon-container";
    LivenessClassNames["RecordingIcon"] = "amplify-liveness-recording-icon";
    LivenessClassNames["StartScreenCameraSelect"] = "amplify-liveness-start-screen-camera-select";
    LivenessClassNames["StartScreenCameraSelectContainer"] = "amplify-liveness-start-screen-camera-select__container";
    LivenessClassNames["StartScreenCameraWaiting"] = "amplify-liveness-start-screen-camera-waiting";
    LivenessClassNames["StartScreenHeader"] = "amplify-liveness-start-screen-header";
    LivenessClassNames["StartScreenHeaderBody"] = "amplify-liveness-start-screen-header__body";
    LivenessClassNames["StartScreenHeaderHeading"] = "amplify-liveness-start-screen-header__heading";
    LivenessClassNames["StartScreenWarning"] = "amplify-liveness-start-screen-warning";
    LivenessClassNames["StartScreenInstructions"] = "amplify-liveness-start-screen-instructions";
    LivenessClassNames["StartScreenInstructionsHeading"] = "amplify-liveness-start-screen-instructions__heading";
    LivenessClassNames["Toast"] = "amplify-liveness-toast";
    LivenessClassNames["ToastContainer"] = "amplify-liveness-toast__container";
    LivenessClassNames["ToastMessage"] = "amplify-liveness-toast__message";
    LivenessClassNames["UserFacingVideo"] = "amplify-liveness-video--user-facing";
    LivenessClassNames["Video"] = "amplify-liveness-video";
    LivenessClassNames["VideoAnchor"] = "amplify-liveness-video-anchor";
})(LivenessClassNames || (LivenessClassNames = {}));

const CancelButton = ({ ariaLabel }) => {
    const [state, send] = useLivenessActor();
    const isFinalState = state.done;
    const handleClick = () => {
        send({
            type: 'CANCEL',
        });
    };
    if (isFinalState)
        return null;
    return (React__namespace["default"].createElement(uiReact.Button, { autoFocus: true, variation: "link", onClick: handleClick, size: "large", className: LivenessClassNames.CancelButton, "aria-label": ariaLabel },
        React__namespace["default"].createElement(internal.IconClose, { "aria-hidden": "true", "data-testid": "close-icon" })));
};

const Toast = ({ variation = 'default', size = 'medium', children, isInitial = false, ...rest }) => {
    const { tokens } = uiReact.useTheme();
    return (React__namespace.createElement(uiReact.View, { className: `${LivenessClassNames.Toast} ${LivenessClassNames.Toast}--${variation} ${LivenessClassNames.Toast}--${size}`, ...(isInitial && { backgroundColor: tokens.colors.background.primary }), ...rest },
        React__namespace.createElement(uiReact.Flex, { className: LivenessClassNames.ToastContainer },
            React__namespace.createElement(uiReact.Flex, { className: LivenessClassNames.ToastMessage, ...(isInitial ? { color: tokens.colors.font.primary } : {}) }, children))));
};

const ToastWithLoader = ({ displayText, }) => {
    return (React__namespace.createElement(Toast, { "aria-live": "polite" },
        React__namespace.createElement(uiReact.Flex, { className: LivenessClassNames.HintText },
            React__namespace.createElement(uiReact.Loader, null),
            React__namespace.createElement(uiReact.View, null, displayText))));
};

const selectErrorState = createLivenessSelector((state) => state.context.errorState);
const selectFaceMatchState$1 = createLivenessSelector((state) => state.context.faceMatchAssociatedParams.faceMatchState);
const selectIlluminationState = createLivenessSelector((state) => state.context.faceMatchAssociatedParams.illuminationState);
const selectIsFaceFarEnoughBeforeRecording = createLivenessSelector((state) => state.context.isFaceFarEnoughBeforeRecording);
const selectFaceMatchStateBeforeStart = createLivenessSelector((state) => state.context.faceMatchStateBeforeStart);
const selectFaceMatchPercentage$1 = createLivenessSelector((state) => state.context.faceMatchAssociatedParams?.faceMatchPercentage);
const DefaultToast = ({ text, isInitial = false, }) => {
    return (React__namespace.createElement(Toast, { size: "large", variation: "primary", isInitial: isInitial },
        React__namespace.createElement(uiReact.View, { "aria-live": "assertive" }, text)));
};
const Hint = ({ hintDisplayText }) => {
    const [state] = useLivenessActor();
    // NOTE: Do not change order of these selectors as the unit tests depend on this order
    const errorState = useLivenessSelector(selectErrorState);
    const faceMatchState = useLivenessSelector(selectFaceMatchState$1);
    const illuminationState = useLivenessSelector(selectIlluminationState);
    const faceMatchStateBeforeStart = useLivenessSelector(selectFaceMatchStateBeforeStart);
    const isFaceFarEnoughBeforeRecordingState = useLivenessSelector(selectIsFaceFarEnoughBeforeRecording);
    const faceMatchPercentage = useLivenessSelector(selectFaceMatchPercentage$1);
    const isCheckFaceDetectedBeforeStart = state.matches('checkFaceDetectedBeforeStart') ||
        state.matches('detectFaceBeforeStart');
    const isCheckFaceDistanceBeforeRecording = state.matches('checkFaceDistanceBeforeRecording') ||
        state.matches('detectFaceDistanceBeforeRecording');
    const isStartView = state.matches('start') || state.matches('userCancel');
    const isRecording = state.matches('recording');
    const isUploading = state.matches('uploading');
    const isCheckSuccessful = state.matches('checkSucceeded');
    const isCheckFailed = state.matches('checkFailed');
    const isFlashingFreshness = state.matches({
        recording: 'flashFreshnessColors',
    });
    const FaceMatchStateStringMap = {
        [FaceMatchState.CANT_IDENTIFY]: hintDisplayText.hintCanNotIdentifyText,
        [FaceMatchState.FACE_IDENTIFIED]: hintDisplayText.hintTooFarText,
        [FaceMatchState.TOO_MANY]: hintDisplayText.hintTooManyFacesText,
        [FaceMatchState.TOO_FAR]: hintDisplayText.hintTooFarText,
        [FaceMatchState.MATCHED]: hintDisplayText.hintHoldFaceForFreshnessText,
        [FaceMatchState.OFF_CENTER]: hintDisplayText.hintFaceOffCenterText,
    };
    const IlluminationStateStringMap = {
        [IlluminationState.BRIGHT]: hintDisplayText.hintIlluminationTooBrightText,
        [IlluminationState.DARK]: hintDisplayText.hintIlluminationTooDarkText,
        [IlluminationState.NORMAL]: hintDisplayText.hintIlluminationNormalText,
    };
    if (isStartView) {
        return (React__namespace.createElement(React__namespace.Fragment, null,
            React__namespace.createElement(uiReact.VisuallyHidden, { role: "alert" }, hintDisplayText.hintCenterFaceInstructionText),
            React__namespace.createElement(DefaultToast, { text: hintDisplayText.hintCenterFaceText, isInitial: true })));
    }
    if (errorState ?? (isCheckFailed || isCheckSuccessful)) {
        return null;
    }
    if (!isRecording) {
        if (isCheckFaceDetectedBeforeStart) {
            if (faceMatchStateBeforeStart === FaceMatchState.TOO_MANY) {
                return React__namespace.createElement(DefaultToast, { text: hintDisplayText.hintTooManyFacesText });
            }
            return (React__namespace.createElement(DefaultToast, { text: hintDisplayText.hintMoveFaceFrontOfCameraText }));
        }
        // Specifically checking for false here because initially the value is undefined and we do not want to show the instruction
        if (isCheckFaceDistanceBeforeRecording &&
            isFaceFarEnoughBeforeRecordingState === false) {
            return React__namespace.createElement(DefaultToast, { text: hintDisplayText.hintTooCloseText });
        }
        if (isUploading) {
            return (React__namespace.createElement(React__namespace.Fragment, null,
                React__namespace.createElement(uiReact.VisuallyHidden, { "aria-live": "assertive" }, hintDisplayText.hintCheckCompleteText),
                React__namespace.createElement(ToastWithLoader, { displayText: hintDisplayText.hintVerifyingText })));
        }
        if (illuminationState && illuminationState !== IlluminationState.NORMAL) {
            return (React__namespace.createElement(DefaultToast, { text: IlluminationStateStringMap[illuminationState] }));
        }
    }
    if (isFlashingFreshness) {
        return React__namespace.createElement(DefaultToast, { text: hintDisplayText.hintHoldFaceForFreshnessText });
    }
    if (isRecording && !isFlashingFreshness) {
        // During face matching, we want to only show the
        // TOO_FAR texts. For FACE_IDENTIFIED, CANT_IDENTIFY, TOO_MANY
        // we are defaulting to the TOO_FAR text (for now).
        let resultHintString = FaceMatchStateStringMap[FaceMatchState.TOO_FAR];
        if (faceMatchState === FaceMatchState.MATCHED) {
            resultHintString = FaceMatchStateStringMap[faceMatchState];
        }
        // If the face is outside the oval set the aria-label to a string about centering face in oval
        let a11yHintString = resultHintString;
        if (faceMatchState === FaceMatchState.OFF_CENTER) {
            a11yHintString = FaceMatchStateStringMap[faceMatchState];
        }
        else if (
        // If the face match percentage reaches 50% append it to the a11y label
        faceMatchState === FaceMatchState.TOO_FAR &&
            faceMatchPercentage > 50) {
            a11yHintString = hintDisplayText.hintMatchIndicatorText;
        }
        return (React__namespace.createElement(Toast, { size: "large", variation: 'primary' },
            React__namespace.createElement(uiReact.VisuallyHidden, { "aria-live": "assertive" }, a11yHintString),
            React__namespace.createElement(uiReact.View, { "aria-label": a11yHintString }, resultHintString)));
    }
    return null;
};

const MatchIndicator = ({ percentage, initialPercentage = 25, testId, }) => {
    const [matchPercentage, setMatchPercentage] = React__namespace["default"].useState(initialPercentage);
    React__namespace["default"].useEffect(() => {
        if (percentage < 0) {
            setMatchPercentage(0);
        }
        else if (percentage > 100) {
            setMatchPercentage(100);
        }
        else {
            setMatchPercentage(percentage);
        }
    }, [percentage]);
    const percentageStyles = {
        '--percentage': `${matchPercentage}%`,
    };
    return (React__namespace["default"].createElement("div", { className: LivenessClassNames.MatchIndicator, "data-testid": testId },
        React__namespace["default"].createElement("div", { className: `${LivenessClassNames.MatchIndicator}__bar`, style: percentageStyles, role: "progressbar", "aria-label": "MatchIndicator", "aria-valuenow": percentage, "aria-valuetext": `${percentage}% face fit` })));
};

const Overlay = ({ children, horizontal = 'center', vertical = 'center', className, ...rest }) => {
    return (React__namespace.createElement(uiReact.Flex, { className: `${LivenessClassNames.Overlay} ${className}`, alignItems: horizontal, justifyContent: vertical, ...rest }, children));
};

const RecordingIcon = ({ children }) => {
    return (React__namespace["default"].createElement(uiReact.Flex, { className: LivenessClassNames.RecordingIcon },
        React__namespace["default"].createElement(uiReact.Flex, { "data-testid": "rec-icon", justifyContent: "center" },
            React__namespace["default"].createElement(uiReact.Icon, { viewBox: { width: 20, height: 20 }, width: "20", height: "20" },
                React__namespace["default"].createElement("circle", { cx: "10", cy: "10", r: "8", fill: "red" }))),
        React__namespace["default"].createElement(uiReact.Text, { as: "span", fontWeight: "bold" }, children)));
};

function isNewerIpad() {
    // iPads on iOS13+ return as if a desktop Mac
    // so check for maxTouchPoints also.
    return (/Macintosh/i.test(navigator.userAgent) &&
        !!navigator.maxTouchPoints &&
        navigator.maxTouchPoints > 1);
}
function isMobileScreen() {
    const isMobileDevice = 
    // Test Android/iPhone/iPad
    /Android|iPhone|iPad/i.test(navigator.userAgent) || isNewerIpad();
    return isMobileDevice;
}
async function isDeviceUserFacing(deviceId) {
    const devices = await navigator.mediaDevices?.enumerateDevices();
    // Find the video input device with the matching deviceId
    const videoDevice = devices?.find((device) => device.deviceId === deviceId && device.kind === 'videoinput');
    if (videoDevice) {
        // Check if the device label contains the word "back"
        return !videoDevice.label.toLowerCase().includes('back');
    }
    // If the device is not found or not a video input device, return false
    return true;
}
/**
 * Use window.matchMedia to direct landscape orientation
 * screen.orientation is not supported in Safari so we will use
 * media query detection to listen for changes instead.
 * @returns MediaQueryList object
 */
function getLandscapeMediaQuery() {
    return window.matchMedia('(orientation: landscape)');
}

const defaultErrorDisplayText = {
    errorLabelText: 'Error',
    connectionTimeoutHeaderText: 'Connection time out',
    connectionTimeoutMessageText: 'Connection has timed out.',
    timeoutHeaderText: 'Time out',
    timeoutMessageText: "Face didn't fit inside oval in time limit. Try again and completely fill the oval with face in it.",
    faceDistanceHeaderText: 'Forward movement detected',
    faceDistanceMessageText: 'Avoid moving closer when connecting.',
    multipleFacesHeaderText: 'Multiple faces detected',
    multipleFacesMessageText: 'Ensure only one face is present in front of the camera when connecting.',
    clientHeaderText: 'Client error',
    clientMessageText: 'Check failed due to client issue',
    serverHeaderText: 'Server issue',
    serverMessageText: 'Cannot complete check due to server issue',
    landscapeHeaderText: 'Landscape orientation not supported',
    landscapeMessageText: 'Rotate your device to portrait (vertical) orientation.',
    portraitMessageText: 'Ensure your device remains in portrait (vertical) orientation for the check’s duration.',
    tryAgainText: 'Try again',
};
const defaultLivenessDisplayText = {
    cameraMinSpecificationsHeadingText: 'Camera does not meet minimum specifications',
    cameraMinSpecificationsMessageText: 'Camera must support at least 320*240 resolution and 15 frames per second.',
    cameraNotFoundHeadingText: 'Camera is not accessible.',
    cameraNotFoundMessageText: 'Check that a camera is connected and there is not another application using the camera. You may have to go into settings to grant camera permissions and close out all instances of your browser and retry.',
    a11yVideoLabelText: 'Webcam for liveness check',
    cancelLivenessCheckText: 'Cancel Liveness check',
    goodFitCaptionText: 'Good fit',
    goodFitAltText: "Ilustration of a person's face, perfectly fitting inside of an oval.",
    hintCenterFaceText: 'Center your face',
    hintCenterFaceInstructionText: 'Instruction: Before starting the check, make sure your camera is at the center top of your screen and center your face to the camera. When the check starts an oval will show up in the center. You will be prompted to move forward into the oval and then prompted to hold still. After holding still for a few seconds, you should hear check complete.',
    hintFaceOffCenterText: 'Face is not in the oval, center your face to the camera.',
    hintMoveFaceFrontOfCameraText: 'Move face in front of camera',
    hintTooManyFacesText: 'Ensure only one face is in front of camera',
    hintFaceDetectedText: 'Face detected',
    hintCanNotIdentifyText: 'Move face in front of camera',
    hintTooCloseText: 'Move back',
    hintTooFarText: 'Move closer',
    hintConnectingText: 'Connecting...',
    hintVerifyingText: 'Verifying...',
    hintCheckCompleteText: 'Check complete',
    hintIlluminationTooBrightText: 'Move to dimmer area',
    hintIlluminationTooDarkText: 'Move to brighter area',
    hintIlluminationNormalText: 'Lighting conditions normal',
    hintHoldFaceForFreshnessText: 'Hold still',
    hintMatchIndicatorText: '50% completed. Keep moving closer.',
    photosensitivityWarningBodyText: 'This check flashes different colors. Use caution if you are photosensitive.',
    photosensitivityWarningHeadingText: 'Photosensitivity warning',
    photosensitivityWarningInfoText: 'Some people may experience epileptic seizures when exposed to colored lights. Use caution if you, or anyone in your family, have an epileptic condition.',
    photosensitivityWarningLabelText: 'More information about photosensitivity',
    photosensitivyWarningBodyText: 'This check flashes different colors. Use caution if you are photosensitive.',
    photosensitivyWarningHeadingText: 'Photosensitivity warning',
    photosensitivyWarningInfoText: 'Some people may experience epileptic seizures when exposed to colored lights. Use caution if you, or anyone in your family, have an epileptic condition.',
    photosensitivyWarningLabelText: 'More information about photosensitivity',
    retryCameraPermissionsText: 'Retry',
    recordingIndicatorText: 'Rec',
    startScreenBeginCheckText: 'Start video check',
    tooFarCaptionText: 'Too far',
    tooFarAltText: "Illustration of a person's face inside of an oval; there is a gap between the perimeter of the face and the boundaries of the oval.",
    waitingCameraPermissionText: 'Waiting for you to allow camera permission.',
    ...defaultErrorDisplayText,
};

const renderToastErrorModal = (props) => {
    const { error: errorState, displayText } = props;
    const { connectionTimeoutHeaderText, connectionTimeoutMessageText, errorLabelText, timeoutHeaderText, timeoutMessageText, faceDistanceHeaderText, faceDistanceMessageText, multipleFacesHeaderText, multipleFacesMessageText, clientHeaderText, clientMessageText, serverHeaderText, serverMessageText, } = displayText;
    let heading;
    let message;
    switch (errorState) {
        case LivenessErrorState.CONNECTION_TIMEOUT:
            heading = connectionTimeoutHeaderText;
            message = connectionTimeoutMessageText;
            break;
        case LivenessErrorState.TIMEOUT:
            heading = timeoutHeaderText;
            message = timeoutMessageText;
            break;
        case LivenessErrorState.FACE_DISTANCE_ERROR:
            heading = faceDistanceHeaderText;
            message = faceDistanceMessageText;
            break;
        case LivenessErrorState.MULTIPLE_FACES_ERROR:
            heading = multipleFacesHeaderText;
            message = multipleFacesMessageText;
            break;
        case LivenessErrorState.RUNTIME_ERROR:
            heading = clientHeaderText;
            message = clientMessageText;
            break;
        case LivenessErrorState.SERVER_ERROR:
        default:
            heading = serverHeaderText;
            message = serverMessageText;
    }
    return (React__namespace["default"].createElement(React__namespace["default"].Fragment, null,
        React__namespace["default"].createElement(uiReact.Flex, { className: LivenessClassNames.ErrorModal },
            React__namespace["default"].createElement(internal.AlertIcon, { ariaLabel: errorLabelText, role: "img", variation: "error" }),
            React__namespace["default"].createElement(uiReact.Text, { className: LivenessClassNames.ErrorModalHeading, id: "amplify-liveness-error-heading" }, heading)),
        React__namespace["default"].createElement(uiReact.Text, { id: "amplify-liveness-error-message" }, message)));
};
const renderErrorModal = ({ errorState, overrideErrorDisplayText, }) => {
    const displayText = {
        ...defaultErrorDisplayText,
        ...overrideErrorDisplayText,
    };
    if (errorState === LivenessErrorState.CAMERA_ACCESS_ERROR ||
        errorState === LivenessErrorState.CAMERA_FRAMERATE_ERROR ||
        errorState === LivenessErrorState.MOBILE_LANDSCAPE_ERROR) {
        return null;
    }
    else {
        return renderToastErrorModal({
            error: errorState,
            displayText,
        });
    }
};
const FaceLivenessErrorModal = (props) => {
    const { children, onRetry, displayText: overrideErrorDisplayText } = props;
    const displayText = {
        ...defaultErrorDisplayText,
        ...overrideErrorDisplayText,
    };
    const { tryAgainText } = displayText;
    return (React__namespace["default"].createElement(Overlay, { className: LivenessClassNames.OpaqueOverlay },
        React__namespace["default"].createElement(Toast, { "aria-labelledby": "amplify-liveness-error-heading", "aria-describedby": "amplify-liveness-error-message", role: "alertdialog" },
            children,
            React__namespace["default"].createElement(uiReact.Flex, { justifyContent: "center" },
                React__namespace["default"].createElement(uiReact.Button, { variation: "primary", type: "button", onClick: onRetry }, tryAgainText)))));
};

/**
 * Copied from src/primitives/Alert/AlertIcon.tsx because we want to re-use the icon but it is not currently expored by AlertIcon.
 * We currently don't want to make a change to the AlertIcon primitive itself and may expose the icon in the future but for now so as not to introduce cross component dependencies we have duplicated it.
 */
const LivenessIconWithPopover = ({ children, headingText, labelText }) => {
    const breakpoint = internal.useThemeBreakpoint();
    const [shouldShowPopover, setShouldShowPopover] = React__namespace.useState(false);
    const wrapperRef = React__namespace.useRef(null);
    const isMobileScreen = breakpoint === 'base';
    React__namespace.useEffect(() => {
        function handleClickOutside(event) {
            if (shouldShowPopover &&
                wrapperRef.current &&
                !wrapperRef.current.contains(event.target)) {
                setShouldShowPopover(false);
            }
        }
        document.addEventListener('mousedown', handleClickOutside);
        return () => {
            document.removeEventListener('mousedown', handleClickOutside);
        };
    }, [wrapperRef, shouldShowPopover]);
    return (React__namespace.createElement("div", { className: LivenessClassNames.Popover, ref: wrapperRef },
        React__namespace.createElement(uiReact.Button, { "aria-controls": "photosensitivity-description", "aria-expanded": shouldShowPopover, role: "alertdialog", "aria-label": labelText, "aria-describedby": "photosensitivity-description", colorTheme: "info", id: "popover-button", onClick: () => setShouldShowPopover(!shouldShowPopover), testId: "popover-icon" },
            React__namespace.createElement(internal.AlertIcon, { ariaHidden: true, variation: "info" })),
        shouldShowPopover && (React__namespace.createElement(React__namespace.Fragment, null,
            React__namespace.createElement(uiReact.Flex, { className: LivenessClassNames.PopoverAnchor }),
            React__namespace.createElement(uiReact.Flex, { className: LivenessClassNames.PopoverAnchorSecondary }),
            React__namespace.createElement(uiReact.Flex, { "aria-hidden": !shouldShowPopover, "aria-label": headingText, className: LivenessClassNames.PopoverContainer, "data-testid": "popover-text", id: "photosensitivity-description", left: isMobileScreen ? -190 : -108, role: "alertdialog" }, children)))));
};
LivenessIconWithPopover.displayName = 'LivenessIconWithPopover';

const DefaultPhotosensitiveWarning = ({ bodyText, headingText, infoText, labelText, }) => {
    return (React__namespace["default"].createElement(uiReact.Flex, { className: `${ui.ComponentClassName.Alert} ${LivenessClassNames.StartScreenWarning}`, style: { zIndex: '3' } },
        React__namespace["default"].createElement(uiReact.View, { flex: "1" },
            React__namespace["default"].createElement(uiReact.View, { className: ui.ComponentClassName.AlertHeading }, headingText),
            React__namespace["default"].createElement(uiReact.View, { className: ui.ComponentClassName.AlertBody }, bodyText)),
        React__namespace["default"].createElement(LivenessIconWithPopover, { labelText: labelText, headingText: headingText }, infoText)));
};
const DefaultRecordingIcon = ({ recordingIndicatorText, }) => {
    return (React__namespace["default"].createElement(uiReact.View, { className: LivenessClassNames.RecordingIconContainer },
        React__namespace["default"].createElement(RecordingIcon, null, recordingIndicatorText)));
};
const DefaultCancelButton = ({ cancelLivenessCheckText, }) => {
    return (React__namespace["default"].createElement(uiReact.View, { className: LivenessClassNames.CancelContainer },
        React__namespace["default"].createElement(CancelButton, { ariaLabel: cancelLivenessCheckText })));
};

const CameraSelector = (props) => {
    const { onSelect: onCameraChange, devices: selectableDevices, deviceId: selectedDeviceId, } = props;
    return (React__namespace["default"].createElement(uiReact.Flex, { className: LivenessClassNames.StartScreenCameraSelect },
        React__namespace["default"].createElement(uiReact.View, { className: LivenessClassNames.StartScreenCameraSelectContainer },
            React__namespace["default"].createElement(uiReact.Label, { htmlFor: "amplify-liveness-camera-select", className: `${LivenessClassNames.StartScreenCameraSelect}__label` }, "Camera:"),
            React__namespace["default"].createElement(uiReact.SelectField, { id: "amplify-liveness-camera-select", testId: "amplify-liveness-camera-select", label: "Camera", labelHidden: true, value: selectedDeviceId, onChange: onCameraChange }, selectableDevices.map((device) => (React__namespace["default"].createElement("option", { value: device.deviceId, key: device.deviceId }, device.label)))))));
};

const selectChallengeType = createLivenessSelector((state) => state.context.parsedSessionInformation?.Challenge?.Name);
const selectVideoConstraints = createLivenessSelector((state) => state.context.videoAssociatedParams?.videoConstraints);
const selectVideoStream = createLivenessSelector((state) => state.context.videoAssociatedParams?.videoMediaStream);
const selectFaceMatchPercentage = createLivenessSelector((state) => state.context.faceMatchAssociatedParams?.faceMatchPercentage);
const selectFaceMatchState = createLivenessSelector((state) => state.context.faceMatchAssociatedParams?.faceMatchState);
const selectSelectedDeviceId = createLivenessSelector((state) => state.context.videoAssociatedParams?.selectedDeviceId);
const selectSelectableDevices = createLivenessSelector((state) => state.context.videoAssociatedParams?.selectableDevices);
const showMatchIndicatorStates = [
    FaceMatchState.TOO_FAR,
    FaceMatchState.CANT_IDENTIFY,
    FaceMatchState.FACE_IDENTIFIED,
    FaceMatchState.OFF_CENTER,
];
/**
 * For now we want to memoize the HOC for MatchIndicator because to optimize renders
 * The LivenessCameraModule still needs to be optimized for re-renders and at that time
 * we should be able to remove this memoization
 */
const MemoizedMatchIndicator = React__namespace["default"].memo(MatchIndicator);
const LivenessCameraModule = (props) => {
    const { isMobileScreen, isRecordingStopped, instructionDisplayText, streamDisplayText, hintDisplayText, errorDisplayText, cameraDisplayText, components: customComponents, testId, } = props;
    const { cancelLivenessCheckText, recordingIndicatorText } = streamDisplayText;
    const { ErrorView = FaceLivenessErrorModal, PhotosensitiveWarning = DefaultPhotosensitiveWarning, } = customComponents ?? {};
    const [state, send] = useLivenessActor();
    const isFaceMovementChallenge = useLivenessSelector(selectChallengeType) === FACE_MOVEMENT_CHALLENGE.type;
    const videoStream = useLivenessSelector(selectVideoStream);
    const videoConstraints = useLivenessSelector(selectVideoConstraints);
    const selectedDeviceId = useLivenessSelector(selectSelectedDeviceId);
    const selectableDevices = useLivenessSelector(selectSelectableDevices);
    const faceMatchPercentage = useLivenessSelector(selectFaceMatchPercentage);
    const faceMatchState = useLivenessSelector(selectFaceMatchState);
    const errorState = useLivenessSelector(selectErrorState);
    const colorMode = internal.useColorMode();
    const { videoRef, videoWidth, videoHeight } = useMediaStreamInVideo(videoStream);
    const canvasRef = React.useRef(null);
    const freshnessColorRef = React.useRef(null);
    const [isCameraReady, setIsCameraReady] = React.useState(false);
    const [isMetadataLoaded, setIsMetadataLoaded] = React.useState(false);
    const [isCameraUserFacing, setIsCameraUserFacing] = React.useState(true);
    const isInitCamera = state.matches('initCamera');
    const isInitWebsocket = state.matches('initWebsocket');
    const isCheckingCamera = state.matches({ initCamera: 'cameraCheck' });
    const isWaitingForCamera = state.matches({
        initCamera: 'waitForDOMAndCameraDetails',
    });
    const isStartView = state.matches('start') || state.matches('userCancel');
    const isDetectFaceBeforeStart = state.matches('detectFaceBeforeStart');
    const isRecording = state.matches('recording');
    const isCheckSucceeded = state.matches('checkSucceeded');
    const isFlashingFreshness = state.matches({
        recording: 'flashFreshnessColors',
    });
    // Android/Firefox and iOS flip the values of width/height returned from
    // getUserMedia, so we'll reset these in useLayoutEffect with the videoRef
    // element's intrinsic videoWidth and videoHeight attributes
    const [mediaWidth, setMediaWidth] = React.useState(videoWidth);
    const [mediaHeight, setMediaHeight] = React.useState(videoHeight);
    const [aspectRatio, setAspectRatio] = React.useState(() => videoWidth && videoHeight ? videoWidth / videoHeight : 0);
    // Only mobile device camera selection for no light challenge
    const hasMultipleDevices = !!selectableDevices?.length && selectableDevices.length > 1;
    const allowDeviceSelection = isStartView &&
        hasMultipleDevices &&
        (!isMobileScreen || isFaceMovementChallenge);
    React__namespace["default"].useEffect(() => {
        async function checkCameraFacing() {
            const isUserFacing = await isDeviceUserFacing(selectedDeviceId);
            setIsCameraUserFacing(isUserFacing);
        }
        checkCameraFacing();
    }, [selectedDeviceId]);
    React__namespace["default"].useEffect(() => {
        const shouldDrawOval = canvasRef?.current &&
            videoRef?.current &&
            videoStream &&
            isStartView &&
            isMetadataLoaded;
        if (shouldDrawOval) {
            drawStaticOval(canvasRef.current, videoRef.current, videoStream);
        }
        const updateColorModeHandler = (e) => {
            if (e.matches && shouldDrawOval) {
                drawStaticOval(canvasRef.current, videoRef.current, videoStream);
            }
        };
        const darkModePreference = window.matchMedia('(prefers-color-scheme: dark)');
        const lightModePreference = window.matchMedia('(prefers-color-scheme: light)');
        darkModePreference.addEventListener('change', updateColorModeHandler);
        lightModePreference.addEventListener('change', updateColorModeHandler);
        return () => {
            darkModePreference.removeEventListener('change', updateColorModeHandler);
            lightModePreference.addEventListener('change', updateColorModeHandler);
        };
    }, [videoRef, videoStream, colorMode, isStartView, isMetadataLoaded]);
    React__namespace["default"].useLayoutEffect(() => {
        if (isCameraReady) {
            send({
                type: 'SET_DOM_AND_CAMERA_DETAILS',
                data: {
                    videoEl: videoRef.current,
                    canvasEl: canvasRef.current,
                    freshnessColorEl: freshnessColorRef.current,
                    isMobile: isMobileScreen,
                },
            });
        }
        if (videoRef.current) {
            setMediaWidth(videoRef.current.videoWidth);
            setMediaHeight(videoRef.current.videoHeight);
            setAspectRatio(videoRef.current.videoWidth / videoRef.current.videoHeight);
        }
    }, [send, videoRef, isCameraReady, isMobileScreen]);
    React__namespace["default"].useEffect(() => {
        if (isDetectFaceBeforeStart) {
            clearOvalCanvas({ canvas: canvasRef.current });
        }
    }, [isDetectFaceBeforeStart]);
    const photoSensitivityWarning = React__namespace["default"].useMemo(() => {
        return (React__namespace["default"].createElement(uiReact.View, { style: { visibility: isStartView ? 'visible' : 'hidden' } },
            React__namespace["default"].createElement(PhotosensitiveWarning, { bodyText: instructionDisplayText.photosensitivityWarningBodyText, headingText: instructionDisplayText.photosensitivityWarningHeadingText, infoText: instructionDisplayText.photosensitivityWarningInfoText, labelText: instructionDisplayText.photosensitivityWarningLabelText })));
    }, [PhotosensitiveWarning, instructionDisplayText, isStartView]);
    const handleMediaPlay = () => {
        setIsCameraReady(true);
    };
    const handleLoadedMetadata = () => {
        setIsMetadataLoaded(true);
    };
    const beginLivenessCheck = React__namespace["default"].useCallback(() => {
        send({
            type: 'BEGIN',
        });
    }, [send]);
    const onCameraChange = React__namespace["default"].useCallback((e) => {
        const newDeviceId = e.target.value;
        const changeCamera = async () => {
            setIsMetadataLoaded(false);
            const newStream = await navigator.mediaDevices.getUserMedia({
                video: {
                    ...videoConstraints,
                    deviceId: { exact: newDeviceId },
                },
                audio: false,
            });
            send({
                type: 'UPDATE_DEVICE_AND_STREAM',
                data: { newDeviceId, newStream },
            });
        };
        changeCamera();
    }, [videoConstraints, send]);
    if (isCheckingCamera) {
        return (React__namespace["default"].createElement(uiReact.Flex, { justifyContent: 'center', className: LivenessClassNames.StartScreenCameraWaiting },
            React__namespace["default"].createElement(uiReact.Loader, { size: "large", className: LivenessClassNames.CenteredLoader, "data-testid": "centered-loader", position: "unset" }),
            React__namespace["default"].createElement(uiReact.Text, { fontSize: "large", fontWeight: "bold", "data-testid": "waiting-camera-permission", className: `${LivenessClassNames.StartScreenCameraWaiting}__text` }, cameraDisplayText.waitingCameraPermissionText)));
    }
    const shouldShowCenteredLoader = isInitCamera || isInitWebsocket;
    // We don't show full screen camera on the pre check screen (isStartView/isWaitingForCamera)
    const shouldShowFullScreenCamera = isMobileScreen && !isStartView && !shouldShowCenteredLoader;
    return (React__namespace["default"].createElement(React__namespace["default"].Fragment, null,
        !isFaceMovementChallenge && photoSensitivityWarning,
        shouldShowCenteredLoader && (React__namespace["default"].createElement(uiReact.Flex, { className: LivenessClassNames.ConnectingLoader },
            React__namespace["default"].createElement(uiReact.Loader, { size: "large", className: LivenessClassNames.Loader, "data-testid": "centered-loader" }),
            React__namespace["default"].createElement(uiReact.Text, { className: LivenessClassNames.LandscapeErrorModalHeader }, hintDisplayText.hintConnectingText))),
        React__namespace["default"].createElement(uiReact.Flex, { className: ui.classNames(LivenessClassNames.CameraModule, shouldShowFullScreenCamera &&
                `${LivenessClassNames.CameraModule}--mobile`), "data-testid": testId, gap: "zero" },
            React__namespace["default"].createElement(Overlay, { horizontal: "center", vertical: isRecording && !isFlashingFreshness ? 'start' : 'space-between', className: LivenessClassNames.InstructionOverlay },
                isRecording && (React__namespace["default"].createElement(DefaultRecordingIcon, { recordingIndicatorText: recordingIndicatorText })),
                !isStartView && !isWaitingForCamera && !isCheckSucceeded && (React__namespace["default"].createElement(DefaultCancelButton, { cancelLivenessCheckText: cancelLivenessCheckText })),
                React__namespace["default"].createElement(uiReact.Flex, { className: ui.classNames(LivenessClassNames.Hint, shouldShowFullScreenCamera && `${LivenessClassNames.Hint}--mobile`) },
                    React__namespace["default"].createElement(Hint, { hintDisplayText: hintDisplayText })),
                errorState && (React__namespace["default"].createElement(ErrorView, { onRetry: () => {
                        send({ type: 'CANCEL' });
                    }, displayText: errorDisplayText }, renderErrorModal({
                    errorState,
                    overrideErrorDisplayText: errorDisplayText,
                }))),
                isRecording &&
                    !isFlashingFreshness &&
                    showMatchIndicatorStates.includes(faceMatchState) ? (React__namespace["default"].createElement(MemoizedMatchIndicator, { percentage: Math.ceil(faceMatchPercentage) })) : null),
            React__namespace["default"].createElement(uiReact.View, { as: "canvas", ref: freshnessColorRef, className: LivenessClassNames.FreshnessCanvas, hidden: true }),
            React__namespace["default"].createElement(uiReact.View, { className: LivenessClassNames.VideoAnchor, style: {
                    aspectRatio: `${aspectRatio}`,
                } },
                React__namespace["default"].createElement("video", { ref: videoRef, muted: true, autoPlay: true, playsInline: true, width: mediaWidth, height: mediaHeight, onCanPlay: handleMediaPlay, onLoadedMetadata: handleLoadedMetadata, "data-testid": "video", className: ui.classNames(LivenessClassNames.Video, isCameraUserFacing && LivenessClassNames.UserFacingVideo, isRecordingStopped && LivenessClassNames.FadeOut), "aria-label": cameraDisplayText.a11yVideoLabelText }),
                React__namespace["default"].createElement(uiReact.Flex, { className: ui.classNames(LivenessClassNames.OvalCanvas, shouldShowFullScreenCamera &&
                        `${LivenessClassNames.OvalCanvas}--mobile`, isRecordingStopped && LivenessClassNames.FadeOut) },
                    React__namespace["default"].createElement(uiReact.View, { as: "canvas", ref: canvasRef })),
                allowDeviceSelection ? (React__namespace["default"].createElement(CameraSelector, { onSelect: onCameraChange, devices: selectableDevices, deviceId: selectedDeviceId })) : null)),
        isStartView && (React__namespace["default"].createElement(uiReact.Flex, { justifyContent: "center" },
            React__namespace["default"].createElement(uiReact.Button, { variation: "primary", type: "button", onClick: beginLivenessCheck }, instructionDisplayText.startScreenBeginCheckText)))));
};

const LandscapeErrorModal = (props) => {
    const { onRetry, header, portraitMessage, landscapeMessage, tryAgainText } = props;
    const [isLandscape, setIsLandscape] = React__namespace.useState(true);
    React__namespace.useLayoutEffect(() => {
        // Get orientation: landscape media query
        const landscapeMediaQuery = getLandscapeMediaQuery();
        // Set ui state for initial orientation
        setIsLandscape(landscapeMediaQuery.matches);
        // Listen for future orientation changes
        landscapeMediaQuery.addEventListener('change', (e) => {
            setIsLandscape(e.matches);
        });
        // Remove matchMedia event listener
        return () => {
            landscapeMediaQuery.removeEventListener('change', (e) => setIsLandscape(e.matches));
        };
    }, []);
    return (React__namespace.createElement(uiReact.Flex, { className: LivenessClassNames.LandscapeErrorModal, height: isLandscape ? 'auto' : 480 },
        React__namespace.createElement(uiReact.Text, { className: LivenessClassNames.LandscapeErrorModalHeader }, header),
        React__namespace.createElement(uiReact.Text, null, isLandscape ? landscapeMessage : portraitMessage),
        !isLandscape ? (React__namespace.createElement(uiReact.Flex, { className: LivenessClassNames.LandscapeErrorModalButton },
            React__namespace.createElement(uiReact.Button, { variation: "primary", type: "button", onClick: onRetry }, tryAgainText))) : null));
};

const CHECK_CLASS_NAME = 'liveness-detector-check';
const CAMERA_ERROR_TEXT_WIDTH = 420;
const selectIsRecordingStopped = createLivenessSelector((state) => state.context.isRecordingStopped);
const LivenessCheck = ({ instructionDisplayText, hintDisplayText, cameraDisplayText, streamDisplayText, errorDisplayText, components, }) => {
    const [state, send] = useLivenessActor();
    const errorState = useLivenessSelector(selectErrorState);
    const isRecordingStopped = useLivenessSelector(selectIsRecordingStopped);
    const isPermissionDenied = state.matches('permissionDenied');
    const isMobile = isMobileScreen();
    const recheckCameraPermissions = () => {
        send({ type: 'RETRY_CAMERA_CHECK' });
    };
    const { cameraMinSpecificationsHeadingText, cameraMinSpecificationsMessageText, cameraNotFoundHeadingText, cameraNotFoundMessageText, retryCameraPermissionsText, } = cameraDisplayText;
    const { cancelLivenessCheckText } = streamDisplayText;
    React__namespace.useLayoutEffect(() => {
        if (isMobile) {
            const sendLandscapeWarning = (isLandscapeMatched) => {
                if (isLandscapeMatched) {
                    send({ type: 'MOBILE_LANDSCAPE_WARNING' });
                }
            };
            // Get orientation: landscape media query
            const landscapeMediaQuery = getLandscapeMediaQuery();
            // Send warning based on initial orientation
            sendLandscapeWarning(landscapeMediaQuery.matches);
            // Listen for future orientation changes and send warning
            landscapeMediaQuery.addEventListener('change', (e) => {
                sendLandscapeWarning(e.matches);
            });
            // Remove matchMedia event listener
            return () => {
                landscapeMediaQuery.removeEventListener('change', (e) => sendLandscapeWarning(e.matches));
            };
        }
    }, [isMobile, send]);
    const renderCheck = () => {
        if (errorState === LivenessErrorState.MOBILE_LANDSCAPE_ERROR) {
            const displayText = {
                ...defaultErrorDisplayText,
                ...errorDisplayText,
            };
            const { landscapeHeaderText, portraitMessageText, landscapeMessageText, tryAgainText, } = displayText;
            return (React__namespace.createElement(uiReact.Flex, { backgroundColor: "background.primary", direction: "column", textAlign: "center", alignItems: "center", justifyContent: "center", position: "absolute", width: "100%" },
                React__namespace.createElement(LandscapeErrorModal, { header: landscapeHeaderText, portraitMessage: portraitMessageText, landscapeMessage: landscapeMessageText, tryAgainText: tryAgainText, onRetry: () => {
                        send({
                            type: 'CANCEL',
                        });
                    } })));
        }
        else if (isPermissionDenied) {
            return (React__namespace.createElement(uiReact.Flex, { backgroundColor: "background.primary", direction: "column", textAlign: "center", alignItems: "center", justifyContent: "center", width: "100%", height: 480 },
                React__namespace.createElement(uiReact.Text, { fontSize: "large", fontWeight: "bold" }, errorState === LivenessErrorState.CAMERA_FRAMERATE_ERROR
                    ? cameraMinSpecificationsHeadingText
                    : cameraNotFoundHeadingText),
                React__namespace.createElement(uiReact.Text, { maxWidth: CAMERA_ERROR_TEXT_WIDTH }, errorState === LivenessErrorState.CAMERA_FRAMERATE_ERROR
                    ? cameraMinSpecificationsMessageText
                    : cameraNotFoundMessageText),
                React__namespace.createElement(uiReact.Button, { variation: "primary", type: "button", onClick: recheckCameraPermissions }, retryCameraPermissionsText),
                React__namespace.createElement(uiReact.View, { position: "absolute", top: "medium", right: "medium" },
                    React__namespace.createElement(CancelButton, { ariaLabel: cancelLivenessCheckText }))));
        }
        else {
            return (React__namespace.createElement(LivenessCameraModule, { isMobileScreen: isMobile, isRecordingStopped: isRecordingStopped, instructionDisplayText: instructionDisplayText, streamDisplayText: streamDisplayText, hintDisplayText: hintDisplayText, errorDisplayText: errorDisplayText, cameraDisplayText: cameraDisplayText, components: components }));
        }
    };
    return (React__namespace.createElement(uiReact.Flex, { direction: "column", position: "relative", testId: CHECK_CLASS_NAME, className: CHECK_CLASS_NAME, gap: "xl" }, renderCheck()));
};

function getMergedDisplayText(overrideDisplayText) {
    const mergeField = (correctKey, deprecatedKey) => {
        if (overrideDisplayText[correctKey] &&
            overrideDisplayText[correctKey] !== defaultLivenessDisplayText[correctKey]) {
            return overrideDisplayText[correctKey];
        }
        else if (overrideDisplayText[deprecatedKey] &&
            overrideDisplayText[deprecatedKey] !==
                defaultLivenessDisplayText[correctKey]) {
            return overrideDisplayText[deprecatedKey];
        }
        else {
            return defaultLivenessDisplayText[correctKey];
        }
    };
    return {
        photosensitivityWarningBodyText: mergeField('photosensitivityWarningBodyText', 'photosensitivyWarningBodyText'),
        photosensitivityWarningHeadingText: mergeField('photosensitivityWarningHeadingText', 'photosensitivyWarningHeadingText'),
        photosensitivityWarningInfoText: mergeField('photosensitivityWarningInfoText', 'photosensitivyWarningInfoText'),
        photosensitivityWarningLabelText: mergeField('photosensitivityWarningLabelText', 'photosensitivyWarningLabelText'),
    };
}
/**
 * Merges optional displayText prop with
 * defaultLivenessDisplayText and returns more bite size portions to pass
 * down to child components of FaceLivenessDetector.
 * @param overrideDisplayText
 * @returns hintDisplayText, cameraDisplayText, instructionDisplayText, cancelLivenessCheckText
 */
function getDisplayText(overrideDisplayText) {
    const mergedDisplayText = getMergedDisplayText(overrideDisplayText ?? {});
    const displayText = {
        ...defaultLivenessDisplayText,
        ...overrideDisplayText,
        ...mergedDisplayText,
    };
    const { a11yVideoLabelText, cameraMinSpecificationsHeadingText, cameraMinSpecificationsMessageText, cameraNotFoundHeadingText, cameraNotFoundMessageText, cancelLivenessCheckText, connectionTimeoutHeaderText, connectionTimeoutMessageText, clientHeaderText, clientMessageText, errorLabelText, hintCanNotIdentifyText, hintCenterFaceText, hintCenterFaceInstructionText, hintFaceOffCenterText, hintConnectingText, hintFaceDetectedText, hintHoldFaceForFreshnessText, hintIlluminationNormalText, hintIlluminationTooBrightText, hintIlluminationTooDarkText, hintMoveFaceFrontOfCameraText, hintTooManyFacesText, hintTooCloseText, hintTooFarText, hintVerifyingText, hintCheckCompleteText, hintMatchIndicatorText, faceDistanceHeaderText, faceDistanceMessageText, goodFitCaptionText, goodFitAltText, landscapeHeaderText, landscapeMessageText, multipleFacesHeaderText, multipleFacesMessageText, photosensitivityWarningBodyText, photosensitivityWarningHeadingText, photosensitivityWarningInfoText, photosensitivityWarningLabelText, photosensitivyWarningBodyText, photosensitivyWarningHeadingText, photosensitivyWarningInfoText, photosensitivyWarningLabelText, portraitMessageText, retryCameraPermissionsText, recordingIndicatorText, serverHeaderText, serverMessageText, startScreenBeginCheckText, timeoutHeaderText, timeoutMessageText, tooFarCaptionText, tooFarAltText, tryAgainText, waitingCameraPermissionText, } = displayText;
    const hintDisplayText = {
        hintMoveFaceFrontOfCameraText,
        hintTooManyFacesText,
        hintFaceDetectedText,
        hintCanNotIdentifyText,
        hintTooCloseText,
        hintTooFarText,
        hintConnectingText,
        hintVerifyingText,
        hintCheckCompleteText,
        hintIlluminationTooBrightText,
        hintIlluminationTooDarkText,
        hintIlluminationNormalText,
        hintHoldFaceForFreshnessText,
        hintCenterFaceText,
        hintCenterFaceInstructionText,
        hintFaceOffCenterText,
        hintMatchIndicatorText,
    };
    const cameraDisplayText = {
        cameraMinSpecificationsHeadingText,
        cameraMinSpecificationsMessageText,
        cameraNotFoundHeadingText,
        cameraNotFoundMessageText,
        retryCameraPermissionsText,
        waitingCameraPermissionText,
        a11yVideoLabelText,
    };
    const instructionDisplayText = {
        photosensitivityWarningBodyText,
        photosensitivityWarningHeadingText,
        photosensitivityWarningInfoText,
        photosensitivityWarningLabelText,
        photosensitivyWarningBodyText,
        photosensitivyWarningHeadingText,
        photosensitivyWarningInfoText,
        photosensitivyWarningLabelText,
        goodFitCaptionText,
        goodFitAltText,
        tooFarCaptionText,
        tooFarAltText,
        startScreenBeginCheckText,
    };
    const streamDisplayText = {
        cancelLivenessCheckText,
        recordingIndicatorText,
    };
    const errorDisplayText = {
        connectionTimeoutHeaderText,
        connectionTimeoutMessageText,
        errorLabelText,
        timeoutHeaderText,
        timeoutMessageText,
        faceDistanceHeaderText,
        faceDistanceMessageText,
        multipleFacesHeaderText,
        multipleFacesMessageText,
        clientHeaderText,
        clientMessageText,
        serverHeaderText,
        serverMessageText,
        landscapeHeaderText,
        landscapeMessageText,
        portraitMessageText,
        tryAgainText,
    };
    return {
        hintDisplayText,
        cameraDisplayText,
        instructionDisplayText,
        streamDisplayText,
        errorDisplayText,
    };
}

const DETECTOR_CLASS_NAME = 'liveness-detector';
function FaceLivenessDetectorCore(props) {
    const { components, config, displayText } = props;
    const currElementRef = React__namespace.useRef(null);
    const { hintDisplayText, cameraDisplayText, instructionDisplayText, streamDisplayText, errorDisplayText, } = getDisplayText(displayText);
    const service = react.useInterpret(livenessMachine, {
        devTools: process.env.NODE_ENV === 'development',
        context: {
            componentProps: {
                ...props,
                config: config ?? {},
            },
        },
    });
    return (React__namespace.createElement(uiReact.View, { className: DETECTOR_CLASS_NAME, testId: DETECTOR_CLASS_NAME },
        React__namespace.createElement(FaceLivenessDetectorProvider, { componentProps: props, service: service },
            React__namespace.createElement(uiReact.Flex, { direction: "column", ref: currElementRef },
                React__namespace.createElement(LivenessCheck, { instructionDisplayText: instructionDisplayText, hintDisplayText: hintDisplayText, cameraDisplayText: cameraDisplayText, streamDisplayText: streamDisplayText, errorDisplayText: errorDisplayText, components: components })))));
}

const credentialProvider = async () => {
    const { credentials } = await auth.fetchAuthSession();
    if (!credentials) {
        throw new Error('No credentials provided');
    }
    return credentials;
};
function FaceLivenessDetector(props) {
    const { config, ...rest } = props;
    return (React__namespace.createElement(FaceLivenessDetectorCore, { ...rest, config: { credentialProvider, ...config } }));
}

exports.FaceLivenessDetector = FaceLivenessDetector;
exports.FaceLivenessDetectorCore = FaceLivenessDetectorCore;
